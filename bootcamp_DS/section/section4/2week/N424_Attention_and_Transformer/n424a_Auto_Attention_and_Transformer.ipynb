{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n424a_Auto_Attention_and_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEHN9J5Bwf5i"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 2 / NOTE 4*\n",
        "\n",
        "# 📝 Assignment\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Attention, Transformer & Others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDZv_Z6Kw3EW"
      },
      "source": [
        "### 문항 1) RNN과 Attention에 대한 설명으로 <ins>옳지 않은</ins> 것을 **모두** 고르시오.\n",
        "\n",
        "1. RNN 기반 모델의 단점 중 하나는 긴 문장(시퀀스)을 처리할 때 앞쪽에 입력된 단어의 의미가 사라지게 되는 장기 의존성(Long-term Dependency) 문제입니다.\n",
        "2. 또한 단어가 순차적으로 입력되기 때문에 연산 시간이 오래 걸린다는 단점을 가지고 있습니다.\n",
        "3. Attention이란 인코더에 입력된 단어와 디코더가 생성하려는 단어가 연관된 정도를 나타내는 가중치입니다.\n",
        "4. 인코더의 입력 벡터와 디코더의 출력 벡터를 내적한 값이 Attention 가중치 값(Score)이 됩니다.\n",
        "5. RNN에 이 Attention을 적용하여 (보기 2번에 해당하는) 연산 시간 문제를 해결할 수 있었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cCyj8DWCVwH"
      },
      "source": [
        "### 문항 2) Transformer에 대한 설명으로 <ins>옳은</ins> 것을 **모두** 고르시오.\n",
        "\n",
        "1. Transformer는 여러 개의 인코더 블록과 디코더 블록으로 이루어진 모델입니다.\n",
        "2. Transformer에는 단어가 순차적으로 입력되기 때문에 위치 정보를 제공해주지 않아도 되는 장점이 있습니다. \n",
        "3. Transformer의 인코더에는 3개의 Sub-layer가 포함되어 있는데 각각 **`Masked Self-Attention`, `Encoder-Decoder Attention`, `Positional Encoding`** 입니다.\n",
        "4. 그 중 인코더 블록에 들어오는 단어 정보와 디코더 블록에 들어오는 단어의 정보를 연결하는 **`Encoder-Decoder Attention`** 입니다. 이 때 디코더 블록의 쿼리(Query)행렬과 인코더 블록의 키(key)행렬, 밸류(value)행렬을 사용합니다.\n",
        "5. **`Masked Self-Attention`** 을 해주는 이유는 해당 위치 오른쪽에 있는 단어를 가림(mask)으로써 Attention 가중치 계산에 사용되지 않도록 하기 위함입니다.\n",
        "6. 자연어처리 연구에서 고안된 Transformer는 자연어가 아닌 다양한 분야에도 사용되고 있습니다.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxD5YTPeG8oP"
      },
      "source": [
        "### 문항 3) Transformer의 Attention 메커니즘 순서를 올바로 나열한 보기를 고르시오.\n",
        "\n",
        "A. 계산하여 나온 값을 (쿼리, 키, 밸류) 벡터 차원의 제곱근에 해당하는 값으로 나누어줍니다.\n",
        "\n",
        "B. 계산하여 나온 값과 밸류 행렬을 곱해줍니다.\n",
        "\n",
        "C. 쿼리 행렬과 키 행렬을 내적합니다.\n",
        "\n",
        "D. 단어 행렬과 준비된 가중치 행렬을 내적하여 쿼리, 키, 밸류 행렬을 만들어냅니다.\n",
        "\n",
        "E. 계산하여 나온 값에 소프트맥스 함수를 취해줍니다.\n",
        "\n",
        "1. D - A - B - E - C\n",
        "2. D - C - A - B - E\n",
        "3. D - C - B - A - E\n",
        "4. D - C - A - E - B\n",
        "5. D - A - B - C - E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZRg-8bMQGsz"
      },
      "source": [
        "### 문항 4) GPT와 BERT에 대한 설명으로 <ins>옳은</ins> 것을 **모두** 고르시오.\n",
        "\n",
        "1. GPT는 Transformer의 디코더 블록을 여러 개 쌓아올린 모델입니다.\n",
        "2. GPT처럼 훈련된 모델을 사전 학습된 언어 모델이라고 하는데, 이런 모델은 Fine-tuning 이후 Pre-training을 하면 특정 태스크에 결과를 얻을 수 있습니다.\n",
        "3. GPT는 양방향으로 단어를 학습할 수 있다는 장점을 바탕으로 몇몇 태스크에서 좋은 성능을 기록하였습니다.\n",
        "4. BERT는 사전 학습에는 MLM, NSP가 있습니다. 이 중 NSP는 `[SEP]` 이라는 스페셜 토큰 양쪽의 텍스트가 원래 이어 붙여져있는 문장인지 아닌지를 판별합니다.\n",
        "5. BERT는 생성과 관련된 태스크에서 강점을 보였지만 자연어 추론(NLI)이나 질의응답(QA)에는 그리 뛰어난 성능을 기록하지 못했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KyHLD9iSqZ5"
      },
      "source": [
        "## 도전 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAbvo7ibUSQQ"
      },
      "source": [
        "- 과제 1 : 주어진 키워드나 문장 중 2개를 선택하여 이해한 대로 블로그에 정리 후 업로드 해주세요. (수업에서 이해한 부분, 구글링을 통해 추가적으로 알아낸 부분, 아직 궁금증이 풀리지 않은 부분을 구분해주시면 더욱 좋습니다.)\n",
        "\n",
        "    - Attention과 장기 의존성\n",
        "    - Self-Attention과 Masked Self-Attention\n",
        "    - Masked Language Model\n",
        "    - Pretrained Language Model\n",
        "\n",
        "\n",
        "- 과제 2 : 오늘 배운 모델 Transformer, BERT, GPT 등에 대한 예제를 찾아 실행해보고 다음 물음에 답하세요. (프레임워크가 사용하는 예제 제외)\n",
        "\n",
        "    - 예제에서 사용된 모델은 무엇인가요?\n",
        "    - 어떤 검색어를 사용하였나요?\n",
        "    - 예제에서는 어떤 데이터를 사용되었나요?\n",
        "    - 전처리 과정에는 어떤 방법이 사용되었나요?\n",
        "    - 모델에 사용된 임베딩 벡터의 차원, 배치 사이즈 등 다양한 설정사항에 대해 적어보세요.\n",
        "    - 혹시 오류가 발생하였다면 어떤 오류가 발생하였으며 어떻게 해결하셨나요?"
      ]
    }
  ]
}