{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "P41_NLP",
      "language": "python",
      "name": "p41_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "n412-vectorization-of-texts.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2shgXaGhA1H"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 1 / NOTE 2*\n",
        "\n",
        "---\n",
        "\n",
        "# Text Vectorization & Documents Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWJIs-m6cZus"
      },
      "source": [
        "# 🏆 학습목표"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfDrzKrHhA1W"
      },
      "source": [
        "* 텍스트 문서를 벡터로 표현해 봅시다\n",
        "* 유사도를 이용해 문서를 검색해 봅시다\n",
        "* 텍스트에서 특성을 추출하고 문서 분류기를 만들 수 있습니다\n",
        "* 잠재의미분석(Latent Semantic Analysis,LSA)을 수행합니다\n",
        "* Spacy 단어 임베딩을 사용합니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRFs_igVhA1W"
      },
      "source": [
        "## Warm up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AK8K-Yseiz-"
      },
      "source": [
        "### TF-IDF\n",
        "\n",
        "다음 동영상을 시청하세요.\n",
        "- [TF-IDF](https://www.youtube.com/watch?v=meEchvkdB1U&feature=youtu.be)\n",
        "  - TF스코어는 어떤 스코어 일까요? \n",
        "  - 왜 IDF 스코어를 추가로 고려할까요? \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NS02N-dgN1i"
      },
      "source": [
        "### SVD\n",
        "다음 동영상을 시청하세요.\n",
        "- [특이값 분해(SVD)의 기하학적 의미와 활용 소개](https://youtu.be/cq5qlYtnLoY)\n",
        "    - 우리는 SVD를 통해 무엇을 얻고자 하는 것일까요?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE1hCjhjitji"
      },
      "source": [
        "### TEXT 분류\n",
        "다음 웹페이지를 읽어보세요. \n",
        "- [Text classification](https://developers.google.com/machine-learning/guides/text-classification)\n",
        "    - Introduction\n",
        "    - Step 1: Gather Data\n",
        "    - Step 2: Explore Your Data\n",
        "    - Step 2.5: Choose a Model\n",
        "    - Step 3: Prepare Your Data\n",
        "\n",
        "전체적인 시각을 늘리는 것이 중요합니다. \n",
        "<br> 페이지를 모두 이해해도 좋겠지만, 내가 앞으로 해나갈 일들을 멀리서 바라볼 수 있도록 준비해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKCHMkiNhA1X"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMP1QpY6hA1Y"
      },
      "source": [
        "## 텍스트 문서를 벡터로 표현해 봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gyzYHuqdMcL"
      },
      "source": [
        "머신러닝 모델에서 텍스트를 사용하기 위해서는 텍스트 데이터를 벡터화해야 합니다. 이것은 텍스트를 컴퓨터가 사용할 수 있게 수치정보로 변환하는 것으로 생각할 수 있습니다. 지난 시간 단어들을 토큰화 했다면, 토큰화한 정보들을 컴퓨터의 언어체계 속으로 넣어준다는 개념으로 받아들이면 됩니다. \n",
        "\n",
        "Bag-of-Words(BoW) 개념은 우리가 사용하는 언어모델을 단순화 시킨 모델입니다. 문서, 문장들에서 문법, 즉 어떤 단어들의 순서 등의 개념을 제거하고 단순히 **단어들의 빈도**만 고려하는 모델입니다.\n",
        "\n",
        "BoW는 문서를 토큰화한 후 토큰의 빈도를 기반으로 벡터화 합니다. 데이터프레임 형태로 보자면 행은 각 문서가 되고 열은 중복되지 않는 각 단어가 됩니다. 열에는 단순히 각 단어가 문서에 얼마나 존재하는지를 카운트한 값을 넣거나(*CountVectorizer* 사용) TF-IDF 값이 오게 할 수 있습니다(*TfidfVectorizer 사용).\n",
        "\n",
        "벡터 표현은 파이썬에서는 `sklearn`, `spacy`패키지를 사용해 구현할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnkwcG3Xf6I2"
      },
      "source": [
        "### BBC Dataset\n",
        "\n",
        "이번 세션에서는 BBC에서 제공하는 데이터셋을 사용하여 텍스트를 다뤄볼 예정입니다. BBC 웹사이트를 방문하는 고객이 방금 읽은 뉴스(문서)를 기반으로 비슷한 다른 문서를 적절하게 추천하게 만들 수 있으면 좋겠죠. 그런 작업을 한번 시작해 봅니다. \n",
        "\n",
        "* 아래 링크에서 파일을 다운로드 받아 노트 폴더에서 압축을 해제합니다. <br> data folder가 생성되고 001.txt ~ 401.txt 파일이 있는지 확인합니다.\n",
        "* colab 유저들은 업로드 코드를 이용하여 업로드 후 unzip하여 파일을 풀면 사용할 수 있습니다.\n",
        "\n",
        "* [bbc_fulltext.zip](https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/bbc_fulltext/bbc_fulltext.zip)\n",
        "\n",
        "\n",
        "* 레퍼런스 - [BBC News Tech]('https://www.bbc.com/news/technology')\n",
        ", D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006. [논문링크](http://mlg.ucd.ie/datasets/bbc.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmN9EYHt54t9"
      },
      "source": [
        "# for Colab User\n",
        "# Upload files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!unzip \"bbc_fulltext.zip\" # 업로드 이름이 다르다면 수정해서 사용하세요."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2ZNkN-36qcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98db0a6-e003-43dd-85b7-7982365bf34d"
      },
      "source": [
        "# data 폴더가 제대로 생성되었는 지 확인합니다. \n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bbc_fulltext.zip  data\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW3iUy8nhA1Y"
      },
      "source": [
        "BoW를 사용해 Document Term Matrices(DTM, 문서-단어행렬)을 만들어 보겠습니다. 각 행은 문서를 나타내고 각 열은 단어를 나타냅니다.\n",
        "- 각 셀의 값은 여러가지 방법으로 표현될 수 있는데\n",
        "    - 단어의 출현 빈도를 나타내거나,\n",
        "    - 단순히 단어의 존재 유무(binary)를 표현할 수 있고,\n",
        "    - TF-IDF 값으로 나타낼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0UrJckqhA1Z"
      },
      "source": [
        "### spacy 예제\n",
        "**Spacy로 텍스트에서 토큰을 추출해 보겠습니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIshXRhDhA1Y"
      },
      "source": [
        "# 모듈에서 사용할 라이브러리와 spacy 모델을 불러옵니다.\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zl8SfWFhA1Z"
      },
      "source": [
        "# 예제로 사용할 Text를 선언합니다. \n",
        "text = \"In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMz6Z1FchA1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc676e0-f020-48fe-873b-eb266dbd2461"
      },
      "source": [
        "# spacy의 언어모델을 이용하여 token화된 단어들을 확인합니다. \n",
        "doc = nlp(text)\n",
        "print([token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['information', 'retrieval', 'tf', 'idf', 'TFIDF', 'short', 'term', 'frequency', 'inverse', 'document', 'frequency', 'numerical', 'statistic', 'intend', 'reflect', 'important', 'word', 'document', 'collection', 'corpus', 'weight', 'factor', 'search', 'information', 'retrieval', 'text', 'mining', 'user', 'modeling', 'tf', 'idf', 'value', 'increase', 'proportionally', 'number', 'time', 'word', 'appear', 'document', 'offset', 'number', 'document', 'corpus', 'contain', 'word', 'help', 'adjust', 'fact', 'word', 'appear', 'frequently', 'general', 'tf', 'idf', 'popular', 'term', 'weight', 'scheme', 'today', 'survey', 'conduct', '2015', 'show', '83', 'text', 'base', 'recommender', 'system', 'digital', 'library', 'use', 'tf', 'idf']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOWp3EEkhA1a"
      },
      "source": [
        "#### BBC Dataset에 적용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiPRiorSq29h"
      },
      "source": [
        "데이터를 축적하는 함수를 제작합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJEsWTtvhA1a"
      },
      "source": [
        "# BBC 데이터를 불러오기 위한 함수\n",
        "import os \n",
        "\n",
        "def gather_data(filefolder):\n",
        "    \"\"\" 폴더 내 텍스트 파일을 각각 리스트 요소에 저장하는 함수\n",
        "    Args:\n",
        "        filefolder (str): .txt 파일이 존재하는 경로\n",
        "    Returns:\n",
        "        문서를 요소로하는 리스트\n",
        "    \"\"\"\n",
        "    \n",
        "    data = []\n",
        "    files = os.listdir(filefolder)\n",
        "    for article in files: \n",
        "        path = os.path.join(filefolder, article)\n",
        "        # txt로 끝나는 파일만 읽습니다\n",
        "        if  path[-3:] == 'txt':\n",
        "            # rb:Read the file in Binary mode\n",
        "            with open(path, 'rb') as f:\n",
        "                data.append(f.read())\n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7i7-H9mhA1a"
      },
      "source": [
        "data = gather_data('./data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcFIgb8rhA1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd50361-3e1e-4f62-ddb7-8fac429dc1ad"
      },
      "source": [
        "# 샘플 확인\n",
        "data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Joke e-mail virus tricks users\\n\\nA virus that disguises itself as a joke is spreading rapidly across the net.\\n\\nAnti-virus firms are issuing high-level warnings about the new version of the Bagle e-mail program that seems to be catching a lot of people out. The Windows virus grabs e-mail addresses from Microsoft Outlook and uses its own mail sending software to spread itself to new victims. When it infects a machine, the Bagle variant turns off security measures that usually protect PCs.\\n\\nThe new variant is called Bagle.AT, Bagle.BB and Bagle.AU and the attachment bearing the virus code is labelled as either \"joke\" or \"price\".\\n\\nThe body of the virus usually contains nothing but a smiley or emoticon. The virus can strike computers running Windows 95, 98, ME, NT, 2000 and XP. Users will be infected if they open the attachment that travels with the e-mail. As well as plundering Microsoft Outlook for e-mail addresses to send itself to, Bagle.AT also tries to turn off the firewall and security centre services on Windows XP machines. BBC News Online has received five warnings about the virus from security companies. Finnish company F-Secure gave the virus its second highest threat level. \"We\\'ve had several reports all over the world,\" said Mikko Hypponen, director of anti-virus research for F-Secure. Security firm Network Box said that it stopped more than 30,000 copies an hour of the virus as the outbreak reached a peak. Black Spider said it had stopped more than 1 million copies of Bagle.AT since the outbreak began at 0630 BST (0530 GMT). Anti-virus firms urged users to be wary of unexpected e-mail messages bearing attachments and to update their software to ensure they are protected against the latest threats.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwyREcsrrQMH"
      },
      "source": [
        "문서별 단어의 수 분포도를 그려봅니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ineq4LzfhA1b"
      },
      "source": [
        "import seaborn as sns\n",
        "# plot 스타일과 폰트 크기를 설정합니다.\n",
        "sns.set(style='whitegrid', font_scale=1.15)\n",
        "\n",
        "# 문서별 단어의 수 분포도 그리는 함수\n",
        "def plot_text_length_dist(text_list):\n",
        "\n",
        "    # 문장이 요소인 리스트를 받아 각 문서의 단어 수를 가진 리스트를 만듭니다\n",
        "    num_words = [len(doc.split()) for doc in text_list]\n",
        "    \n",
        "    sns.displot(num_words)\n",
        "    plt.title('# of words per documents')\n",
        "    plt.xlabel('Number of words')\n",
        "    plt.ylabel('Number of documents')\n",
        "    plt.show()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juxrcqHDhA1b"
      },
      "source": [
        "대략 500 단어 정도로 표현된 문서가 가장 많이 보입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riyI9CVThA1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "fb4ed304-3228-459a-a12f-850b1a272846"
      },
      "source": [
        "plot_text_length_dist(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iW_tDfwhA1c"
      },
      "source": [
        "### CountVectorizer 예제\n",
        ": 단어들의 출현 빈도로 여러개의 문서를 벡터화하는 함수 <br>\n",
        ": 모든 문자를 소문자로 전환하여 계산함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQpwlq6ghA1c"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# wiki 문장들을 리스트에 나누어 입력해봅니다. \n",
        "text = [\"In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\"\n",
        ",\"It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\"\n",
        ",\"The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\"\n",
        ",\"tf–idf is one of the most popular term-weighting schemes today.\"\n",
        ",\"A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.\"]\n",
        "\n",
        "# CountVectorizer 생성\n",
        "vect = CountVectorizer()\n",
        "\n",
        "# text를 기반으로 어휘 사전을 생성\n",
        "vect.fit(text) \n",
        "# vect.fit(data[:5])\n",
        "\n",
        "# text를 DTM(document-term matrix)으로 변환(transform)\n",
        "dtm = vect.transform(text)\n",
        "# dtm = vect.transform(data[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYo0jODwhA1c"
      },
      "source": [
        "vocabulary(모든 토큰)와 맵핑된 인덱스 정보를 확인할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1EtyoQchA1c"
      },
      "source": [
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZLNl0pUhA1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b01ec8-6032-4eba-876f-bb63a626e559"
      },
      "source": [
        "dtm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj4dr7UvhA1d"
      },
      "source": [
        "추출된 토큰을 나열해 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_034SHvhA1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7971d1-4993-4d8b-82df-e2b9ea2e0cbc"
      },
      "source": [
        "print(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2015', '83', 'adjust', 'and', 'appear', 'appears', 'as', 'based', 'by', 'collection', 'conducted', 'contain', 'corpus', 'digital', 'document', 'documents', 'fact', 'factor', 'for', 'frequency', 'frequently', 'general', 'helps', 'how', 'idf', 'important', 'in', 'increases', 'information', 'intended', 'inverse', 'is', 'it', 'libraries', 'mining', 'modeling', 'more', 'most', 'number', 'numerical', 'of', 'offset', 'often', 'one', 'or', 'popular', 'proportionally', 'recommender', 'reflect', 'retrieval', 'schemes', 'searches', 'short', 'showed', 'some', 'statistic', 'survey', 'systems', 'term', 'text', 'tf', 'tfidf', 'that', 'the', 'times', 'to', 'today', 'use', 'used', 'user', 'value', 'weighting', 'which', 'word', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo8ojHRBhA1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5fa252-0651-4fac-9fed-b12f5d1b66b8"
      },
      "source": [
        "text[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Mobiles rack up 20 years of use\\n\\nMobile phones in the UK are celebrating their 20th anniversary this weekend.\\n\\nBritain\\'s first mobile phone call was made across the Vodafone network on 1 January 1985 by veteran comedian Ernie Wise. In the 20 years since that day, mobile phones have become an integral part of modern life and now almost 90% of Britons own a handset. Mobiles have become so popular that many people use their handset as their only phone and rarely use a landline.\\n\\nThe first ever call over a portable phone was made in 1973 in New York but it took 10 years for the first commercial mobile service to be launched. The UK was not far behind the rest of the world in setting up networks in 1985 that let people make calls while they walked. The first call was made from St Katherine\\'s dock to Vodafone\\'s head office in Newbury which at the time was over a curry house. For the first nine days of 1985 Vodafone was the only firm with a mobile network in the UK. Then on 10 January Cellnet (now O2) launched its service. Mike Caudwell, spokesman for Vodafone, said that when phones were launched they were the size of a briefcase, cost about \\xc2\\xa32,000 and had a battery life of little more than 20 minutes.\\n\\n\"Despite that they were hugely popular in the mid-80s,\" he said. \"They became a yuppy must-have and a status symbol among young wealthy business folk.\" This was also despite the fact that the phones used analogue radio signals to communicate which made them very easy to eavesdrop on. He said it took Vodafone almost nine years to rack up its first million customers but only 18 months to get the second million. \"It\\'s very easy to forget that in 1983 when we put the bid document in we were forecasting that the total market would be two million people,\" he said. \"Cellnet was forecasting half that.\" Now Vodafone has 14m customers in the UK alone. Cellnet and Vodafone were the only mobile phone operators in the UK until 1993 when One2One (now T-Mobile) was launched. Orange had its UK launch in 1994. Both newcomers operated digital mobile networks and now all operators use this technology. The analogue spectrum for the old phones has been retired. Called Global System for Mobiles (GSM) this is now the most widely used phone technology on the planet and is used to help more than 1.2 billion people make calls. Mr Caudwell said the advent of digital technology also helped to introduce all those things, such as text messaging and roaming that have made mobiles so popular.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUj5YBA0hA1d"
      },
      "source": [
        "dtm의 타입을 보면 compressed sparse Row matrix임을 알 수 있습니다. <br> csr: Compressed Sparse Row matrix, sparse matrix 형태에서 0을 표현하지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EonNdMuvhA1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8686bc2a-a8f1-4a3e-e8c6-8c5503a88d1d"
      },
      "source": [
        "# CountVectorizer 로 제작한 dtm을 분석\n",
        "type(dtm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j_S9jAYhA1e"
      },
      "source": [
        "# (row, column)  count\n",
        "print(dtm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcfHPTRrhA1e"
      },
      "source": [
        "0을 표현한 형태로 만들면 .todense를 사용할 수 있고, 이런경우 numpy.matrix형태가 됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz3m9i8ahA1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc0d265-e748-4781-9a58-eb22d9f38498"
      },
      "source": [
        "# Return a dense matrix representation\n",
        "# dtm.todense()\n",
        "print(type(dtm))\n",
        "print(type(dtm.todense()))\n",
        "dtm.todense()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "<class 'numpy.matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZaCXze6hA1e"
      },
      "source": [
        "데이터프레임 형태로 결과를 보고 싶다면"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5h5m4FHhA1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "785d5c53-9891-45a9-a562-28d2c6ed59e6"
      },
      "source": [
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
        "print(type(dtm))\n",
        "dtm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2015</th>\n",
              "      <th>83</th>\n",
              "      <th>adjust</th>\n",
              "      <th>and</th>\n",
              "      <th>appear</th>\n",
              "      <th>appears</th>\n",
              "      <th>as</th>\n",
              "      <th>based</th>\n",
              "      <th>by</th>\n",
              "      <th>collection</th>\n",
              "      <th>conducted</th>\n",
              "      <th>contain</th>\n",
              "      <th>corpus</th>\n",
              "      <th>digital</th>\n",
              "      <th>document</th>\n",
              "      <th>documents</th>\n",
              "      <th>fact</th>\n",
              "      <th>factor</th>\n",
              "      <th>for</th>\n",
              "      <th>frequency</th>\n",
              "      <th>frequently</th>\n",
              "      <th>general</th>\n",
              "      <th>helps</th>\n",
              "      <th>how</th>\n",
              "      <th>idf</th>\n",
              "      <th>important</th>\n",
              "      <th>in</th>\n",
              "      <th>increases</th>\n",
              "      <th>information</th>\n",
              "      <th>intended</th>\n",
              "      <th>inverse</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>libraries</th>\n",
              "      <th>mining</th>\n",
              "      <th>modeling</th>\n",
              "      <th>more</th>\n",
              "      <th>most</th>\n",
              "      <th>number</th>\n",
              "      <th>numerical</th>\n",
              "      <th>of</th>\n",
              "      <th>offset</th>\n",
              "      <th>often</th>\n",
              "      <th>one</th>\n",
              "      <th>or</th>\n",
              "      <th>popular</th>\n",
              "      <th>proportionally</th>\n",
              "      <th>recommender</th>\n",
              "      <th>reflect</th>\n",
              "      <th>retrieval</th>\n",
              "      <th>schemes</th>\n",
              "      <th>searches</th>\n",
              "      <th>short</th>\n",
              "      <th>showed</th>\n",
              "      <th>some</th>\n",
              "      <th>statistic</th>\n",
              "      <th>survey</th>\n",
              "      <th>systems</th>\n",
              "      <th>term</th>\n",
              "      <th>text</th>\n",
              "      <th>tf</th>\n",
              "      <th>tfidf</th>\n",
              "      <th>that</th>\n",
              "      <th>the</th>\n",
              "      <th>times</th>\n",
              "      <th>to</th>\n",
              "      <th>today</th>\n",
              "      <th>use</th>\n",
              "      <th>used</th>\n",
              "      <th>user</th>\n",
              "      <th>value</th>\n",
              "      <th>weighting</th>\n",
              "      <th>which</th>\n",
              "      <th>word</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   2015  83  adjust  and  appear  ...  value  weighting  which  word  words\n",
              "0     0   0       0    0       0  ...      0          0      0     1      0\n",
              "1     0   0       0    1       0  ...      0          1      0     0      0\n",
              "2     0   0       1    1       1  ...      1          0      1     2      1\n",
              "3     0   0       0    0       0  ...      0          1      0     0      0\n",
              "4     1   1       0    0       0  ...      0          0      0     0      0\n",
              "\n",
              "[5 rows x 75 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGuj6KyThA1e"
      },
      "source": [
        "세번째 문장과, dtm을 비교해 보겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd-D524WhA1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4c93c8bc-47d3-4dee-e57d-00704a2c88df"
      },
      "source": [
        "text[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAzTOSWOhA1f"
      },
      "source": [
        "#### BBC dataset에 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9pVcwbRhA1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64605ba0-f22b-4c6c-9909-6b4b9d60e61f"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## stop_words = 'english' 영어에 해당하는 불용어 처리를 합니다.\n",
        "## max_features=n, 빈도 순서대로 top n 단어만 사용합니다.\n",
        "vect = CountVectorizer(stop_words='english'\n",
        "                       , max_features=10000)\n",
        "# fit & transform\n",
        "dtm = vect.fit_transform(data)\n",
        "\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
        "dtm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsgyNcfHhA1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "5226fdb5-4cc6-4d86-90f5-25b6001d2c7f"
      },
      "source": [
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>000</th>\n",
              "      <th>000s</th>\n",
              "      <th>0051</th>\n",
              "      <th>007</th>\n",
              "      <th>01</th>\n",
              "      <th>028</th>\n",
              "      <th>04m</th>\n",
              "      <th>05</th>\n",
              "      <th>0530</th>\n",
              "      <th>056</th>\n",
              "      <th>0630</th>\n",
              "      <th>080</th>\n",
              "      <th>0800</th>\n",
              "      <th>0870</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>100m</th>\n",
              "      <th>100s</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>104</th>\n",
              "      <th>106</th>\n",
              "      <th>106cm</th>\n",
              "      <th>1080</th>\n",
              "      <th>10cm</th>\n",
              "      <th>10m</th>\n",
              "      <th>10s</th>\n",
              "      <th>10th</th>\n",
              "      <th>10x7in</th>\n",
              "      <th>11</th>\n",
              "      <th>110</th>\n",
              "      <th>115</th>\n",
              "      <th>117</th>\n",
              "      <th>11b</th>\n",
              "      <th>11m</th>\n",
              "      <th>12</th>\n",
              "      <th>120</th>\n",
              "      <th>120bn</th>\n",
              "      <th>...</th>\n",
              "      <th>yeob</th>\n",
              "      <th>yepp</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yeun</th>\n",
              "      <th>yh</th>\n",
              "      <th>yle</th>\n",
              "      <th>yoda</th>\n",
              "      <th>yoga</th>\n",
              "      <th>yonca</th>\n",
              "      <th>yoran</th>\n",
              "      <th>york</th>\n",
              "      <th>yorker</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youngsters</th>\n",
              "      <th>youth</th>\n",
              "      <th>yuppy</th>\n",
              "      <th>yusuf</th>\n",
              "      <th>zafi</th>\n",
              "      <th>zander</th>\n",
              "      <th>zar</th>\n",
              "      <th>zdnet</th>\n",
              "      <th>zealous</th>\n",
              "      <th>zed</th>\n",
              "      <th>zelda</th>\n",
              "      <th>zen</th>\n",
              "      <th>zenith</th>\n",
              "      <th>zens</th>\n",
              "      <th>zero</th>\n",
              "      <th>zhang</th>\n",
              "      <th>zip</th>\n",
              "      <th>zodiac</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombies</th>\n",
              "      <th>zone</th>\n",
              "      <th>zonealarm</th>\n",
              "      <th>zones</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zooms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 10000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   00  000  000s  0051  007  01  ...  zombies  zone  zonealarm  zones  zoom  zooms\n",
              "0   0    1     0     0    0   0  ...        0     0          0      0     0      0\n",
              "1   0    0     0     0    0   0  ...        0     0          0      0     0      0\n",
              "2   0    1     0     0    0   0  ...        0     0          0      0     0      0\n",
              "3   0    0     0     0    0   0  ...        0     0          0      0     0      0\n",
              "4   0    1     0     0    0   0  ...        0     0          0      0     0      0\n",
              "\n",
              "[5 rows x 10000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp_BuYEYhA1f"
      },
      "source": [
        "### TfidfVectorizer 예제\n",
        "\n",
        "Term Frequency - Inverse Document Frequency ([TF-IDF](https://mungingdata.wordpress.com/2017/11/25/episode-1-using-tf-idf-to-identify-the-signal-from-the-noise/), 단어빈도-역문서빈도)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFv0E6ZIhA1g"
      },
      "source": [
        "Warmup 보았던 영상의 내용을 정리해봅시다. \n",
        "\n",
        "-  TF(Term Frequency, 단어의 빈도) : 문서에서 단어가 많이 등장하는 지를 수치화한 중요도 스코어(score)\n",
        " <br> **특정 문서 d에서 특정 단어 t가 쓰인 빈도**:\n",
        "\n",
        "$\\large tf(t,d) = \\frac{Term\\; t\\; frequency\\; in\\; document}{Total\\; words\\; in\\; document}$\n",
        "\n",
        "`ex) \"A new car, used car, car reivew\" `\n",
        "  - TF score :  A($1 \\over 7$), new($1 \\over 7$), car($3 \\over 7$), used($1 \\over 7$), reivew($1 \\over 7$)  \n",
        "  - IDF - $log {(총 \\space 문장 \\space 개수) \\over {(단어가 \\space 출현한 \\space 문장의 \\space 개수)}}$\n",
        "  $\\large idf(t) = log(\\frac{n}{1+df(t)})$\n",
        "<br></br>\n",
        "  \n",
        "`ex) 'A'가 모든 문장에서 등장 IDF = log(N/N) = 0`<br>\n",
        "\n",
        "- TF-IDF Score \n",
        "<br> $ = \\large tf(t,d) \\times idf(t)$\n",
        "  <br> (t=단어, d=문서, n=총 문서수*)\n",
        "\n",
        "`ex) TF-IDF score = TF * IDF = (1/7) * 0 = 0`\n",
        "\n",
        "\n",
        "TF-IDF 를 사용하는 이유는 문서를 구분하는데 어떤 단어가 중요한지(**unique**) 찾는 것 입니다.\n",
        "\n",
        "수식을 살펴보면, \n",
        "<br> 여러 문서에서 많이 등장하는 단어일 수록 중요도가 낮다고 판단하며(IDF), <br> 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다(TF).\n",
        "\n",
        "TF-IDF를 통한 자연어 처리는 간단하고 빠르게 구현할 수 있으므로 좋은 Baseline으로 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg2e_5NZsbiv"
      },
      "source": [
        "#### TF-IDF vs Count vectorizer \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceXM5l7yhA1g"
      },
      "source": [
        "[1] TF-IDF(Tfidf) vectorizer를 생성하고 Document-Term Matrix (DTM)을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhHOIOh_hA1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "dc2e0399-f06e-473f-a0c9-fe15ea269c31"
      },
      "source": [
        "# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한하였습니다.\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=15)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "dtm = tfidf.fit_transform(text)\n",
        "\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
        "dtm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>document</th>\n",
              "      <th>frequency</th>\n",
              "      <th>idf</th>\n",
              "      <th>information</th>\n",
              "      <th>number</th>\n",
              "      <th>recommender</th>\n",
              "      <th>reflect</th>\n",
              "      <th>retrieval</th>\n",
              "      <th>searches</th>\n",
              "      <th>term</th>\n",
              "      <th>text</th>\n",
              "      <th>tf</th>\n",
              "      <th>weighting</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.237982</td>\n",
              "      <td>0.475965</td>\n",
              "      <td>0.589946</td>\n",
              "      <td>0.166183</td>\n",
              "      <td>0.237982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.294973</td>\n",
              "      <td>0.237982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.237982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166183</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.237982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425001</td>\n",
              "      <td>0.526778</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.276074</td>\n",
              "      <td>0.276074</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.192782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.684374</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.192782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.552149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.404837</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.579748</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.404837</td>\n",
              "      <td>0.579748</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.372642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.661438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533644</td>\n",
              "      <td>0.372642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     corpus  document  frequency  ...        tf  weighting      word\n",
              "0  0.237982  0.475965   0.589946  ...  0.166183   0.000000  0.237982\n",
              "1  0.000000  0.000000   0.000000  ...  0.000000   0.425001  0.000000\n",
              "2  0.276074  0.276074   0.000000  ...  0.192782   0.000000  0.552149\n",
              "3  0.000000  0.000000   0.000000  ...  0.404837   0.579748  0.000000\n",
              "4  0.000000  0.000000   0.000000  ...  0.372642   0.000000  0.000000\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqNXLX1yhA1h"
      },
      "source": [
        "[2] 같은 파라미터로 CountVectorizer를 사용해 tfidf 결과와 비교해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfy8eVKOhA1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "d24488e0-5110-4f57-e010-ed24316a15a7"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english', max_features=15)\n",
        "dtm = vect.fit_transform(text)\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
        "dtm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>document</th>\n",
              "      <th>frequency</th>\n",
              "      <th>idf</th>\n",
              "      <th>information</th>\n",
              "      <th>number</th>\n",
              "      <th>recommender</th>\n",
              "      <th>reflect</th>\n",
              "      <th>retrieval</th>\n",
              "      <th>searches</th>\n",
              "      <th>term</th>\n",
              "      <th>text</th>\n",
              "      <th>tf</th>\n",
              "      <th>weighting</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   corpus  document  frequency  idf  ...  text  tf  weighting  word\n",
              "0       1         2          2    1  ...     0   1          0     1\n",
              "1       0         0          0    0  ...     1   0          1     0\n",
              "2       1         1          0    1  ...     0   1          0     2\n",
              "3       0         0          0    1  ...     0   1          1     0\n",
              "4       0         0          0    1  ...     1   1          0     0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7nUg4rphA1h"
      },
      "source": [
        "#### BBC Dataset에 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHnIK0uqhA1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "6cc7c1fa-1bd3-49a7-ea10-92f016eba9b2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "dtm = tfidf.fit_transform(data)\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>104</th>\n",
              "      <th>10m</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>120</th>\n",
              "      <th>13</th>\n",
              "      <th>130</th>\n",
              "      <th>14</th>\n",
              "      <th>149</th>\n",
              "      <th>15</th>\n",
              "      <th>150</th>\n",
              "      <th>16</th>\n",
              "      <th>167</th>\n",
              "      <th>17</th>\n",
              "      <th>17m</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>1980s</th>\n",
              "      <th>1982</th>\n",
              "      <th>1983</th>\n",
              "      <th>1985</th>\n",
              "      <th>1990s</th>\n",
              "      <th>1993</th>\n",
              "      <th>1994</th>\n",
              "      <th>1995</th>\n",
              "      <th>1bn</th>\n",
              "      <th>1m</th>\n",
              "      <th>20</th>\n",
              "      <th>200</th>\n",
              "      <th>2000</th>\n",
              "      <th>2001</th>\n",
              "      <th>2002</th>\n",
              "      <th>2003</th>\n",
              "      <th>2004</th>\n",
              "      <th>2005</th>\n",
              "      <th>2006</th>\n",
              "      <th>...</th>\n",
              "      <th>worry</th>\n",
              "      <th>worrying</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>worthwhile</th>\n",
              "      <th>wouldn</th>\n",
              "      <th>wow</th>\n",
              "      <th>wright</th>\n",
              "      <th>wristwatch</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writers</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrongful</th>\n",
              "      <th>wrote</th>\n",
              "      <th>wsis</th>\n",
              "      <th>x1</th>\n",
              "      <th>xbox</th>\n",
              "      <th>xenon</th>\n",
              "      <th>xp</th>\n",
              "      <th>xxx</th>\n",
              "      <th>yahoo</th>\n",
              "      <th>yang</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yen</th>\n",
              "      <th>yes</th>\n",
              "      <th>yoda</th>\n",
              "      <th>yoran</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youngsters</th>\n",
              "      <th>youth</th>\n",
              "      <th>zafi</th>\n",
              "      <th>zen</th>\n",
              "      <th>zombies</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.026072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.051915</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102341</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.066212</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.034386</td>\n",
              "      <td>0.079190</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.087326</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.027346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028192</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.028940</td>\n",
              "      <td>0.066646</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073493</td>\n",
              "      <td>0.241386</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.071006</td>\n",
              "      <td>0.067081</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.122338</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.123824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05528</td>\n",
              "      <td>0.058505</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        000        10  100  101  104  ...  youngsters  youth  zafi  zen  zombies\n",
              "0  0.026072  0.000000  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "1  0.000000  0.000000  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "2  0.034386  0.079190  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "3  0.000000  0.000000  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "4  0.028940  0.066646  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "\n",
              "[5 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoiVe1n7hA1h"
      },
      "source": [
        "#### 파라미터 튜닝\n",
        "\n",
        "> 이번에는 조금 더 파라미터를 튜닝하고, spacy tokenizer 를 사용해서 벡터화를 진행해 보겠습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Oq9Yq4hA1i"
      },
      "source": [
        "# spacy tokenizer 함수\n",
        "def tokenize(document):\n",
        "    \n",
        "    doc = nlp(document)\n",
        "    # punctuations: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.is_alpha == True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wOsy1gVhA1i"
      },
      "source": [
        "파라미터 튜닝을 더 해보겠습니다. 여러 파라미터들을 변경해 가며 결과를 비교해 보십시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa8za2l8hA1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "e85dc98b-51fa-4950-9361-78903c8b50d6"
      },
      "source": [
        "# ngram_range = (min_n, max_n), min_n 개~ max_n 개를 갖는 n-gram(n개의 연속적인 토큰)을 토큰으로 사용합니다.\n",
        "# min_df = n, 최소 n개의 문서에 나타나는 토큰만 사용합니다\n",
        "# max_df = .7, 70% 이상 문서에 나타나는 토큰은 제거합니다\n",
        "tfidf = TfidfVectorizer(stop_words='english'\n",
        "                        ,tokenizer=tokenize\n",
        "                        ,ngram_range=(1,2)\n",
        "                        ,max_df=.7\n",
        "                        ,min_df=3\n",
        "#                         ,max_features = 4000\n",
        "                       )\n",
        "\n",
        "dtm = tfidf.fit_transform(data)\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-PRON-</th>\n",
              "      <th>abandon</th>\n",
              "      <th>ability</th>\n",
              "      <th>ability record</th>\n",
              "      <th>able</th>\n",
              "      <th>able access</th>\n",
              "      <th>able choose</th>\n",
              "      <th>able control</th>\n",
              "      <th>able handle</th>\n",
              "      <th>able offer</th>\n",
              "      <th>able play</th>\n",
              "      <th>able store</th>\n",
              "      <th>able thing</th>\n",
              "      <th>able use</th>\n",
              "      <th>able watch</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absorb</th>\n",
              "      <th>abuse</th>\n",
              "      <th>academic</th>\n",
              "      <th>academy</th>\n",
              "      <th>accept</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>acceptance</th>\n",
              "      <th>access</th>\n",
              "      <th>access available</th>\n",
              "      <th>access datum</th>\n",
              "      <th>access device</th>\n",
              "      <th>access e</th>\n",
              "      <th>access grow</th>\n",
              "      <th>access home</th>\n",
              "      <th>access information</th>\n",
              "      <th>access internet</th>\n",
              "      <th>access medium</th>\n",
              "      <th>access mobile</th>\n",
              "      <th>access net</th>\n",
              "      <th>access point</th>\n",
              "      <th>access service</th>\n",
              "      <th>accessible</th>\n",
              "      <th>accident</th>\n",
              "      <th>acclaim</th>\n",
              "      <th>...</th>\n",
              "      <th>year launch</th>\n",
              "      <th>year microsoft</th>\n",
              "      <th>year million</th>\n",
              "      <th>year motorola</th>\n",
              "      <th>year new</th>\n",
              "      <th>year number</th>\n",
              "      <th>year old</th>\n",
              "      <th>year people</th>\n",
              "      <th>year portable</th>\n",
              "      <th>year predict</th>\n",
              "      <th>year real</th>\n",
              "      <th>year release</th>\n",
              "      <th>year report</th>\n",
              "      <th>year say</th>\n",
              "      <th>year service</th>\n",
              "      <th>year think</th>\n",
              "      <th>year time</th>\n",
              "      <th>year uk</th>\n",
              "      <th>year use</th>\n",
              "      <th>year year</th>\n",
              "      <th>yen</th>\n",
              "      <th>yepp</th>\n",
              "      <th>yes</th>\n",
              "      <th>york</th>\n",
              "      <th>york base</th>\n",
              "      <th>york state</th>\n",
              "      <th>york times</th>\n",
              "      <th>young</th>\n",
              "      <th>young americans</th>\n",
              "      <th>young people</th>\n",
              "      <th>young user</th>\n",
              "      <th>youngster</th>\n",
              "      <th>youth</th>\n",
              "      <th>zelda</th>\n",
              "      <th>zen</th>\n",
              "      <th>zen micro</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombie bot</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.068952</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.074656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.052588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.049556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7604 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   -PRON-  abandon   ability  ability record  ...  zombie  zombie bot  zone  zoom\n",
              "0     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "1     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "2     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "3     0.0      0.0  0.024268             0.0  ...     0.0         0.0   0.0   0.0\n",
              "4     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 7604 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2fjGZhRhA1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72c1924-b80b-4e5d-dde3-334c1c34e184"
      },
      "source": [
        "dtm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 7604)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76FurcFFhA1j"
      },
      "source": [
        "## 유사도를 이용해 문서를 검색해 봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyZVc10EhA1j"
      },
      "source": [
        "네이버, 구글과 같은 검색엔진의 원리는 무엇입니까? \n",
        "<br> 검색어를 인터넷에 존재하는 여러 문서들과 단순히 같은지만 비교 하는것은 아닙니다. 쿼리와 문서들을 매칭(matching) 하는 방법은 여러가지가 있습니다. 그중 가장 클래식한 방법인 \"유사도 측정 방법\"을 시도해 봅니다. \n",
        "<br> 이를 위해 n-차원 거리를 사용하는 방법을 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "przZffV_hA1j"
      },
      "source": [
        "### 코사인 유사도(Cosine Similarity, Brute Force 방법)\n",
        "\n",
        "$\\Large similarity=cos(Θ)=\\frac{A⋅B}{||A||\\ ||B||}=\\frac{\\sum_{i=1}^{n}{A_{i}×B_{i}}}{\\sqrt{\\sum_{i=1}^{n}(A_{i})^2}×\\sqrt{\\sum_{i=1}^{n}(B_{i})^2}}$\n",
        "\n",
        "<img align=\"center\" src=\"https://images.deepai.org/glossary-terms/cosine-similarity-1007790.jpg\" width=700 title=\"Cosine Similarity\" alt=\"https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity\">\n",
        "\n",
        "\n",
        "코사인 유사도는 두 벡터(문서벡터) 간의 각의 코사인 값을 이용하여 구할 수 있는 유사도 입니다.\n",
        "- 두 벡터(문서)가 \n",
        "    - 완전히 같을 경우 1이며\n",
        "    - 90도의 각을 이루면 0\n",
        "    - 완전히 반대방향을 이루면 -1 입니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nruxP2slhA1j"
      },
      "source": [
        "#### TF-IDF 벡터 거리\n",
        "TF-IDF 벡터들의 거리를 계산해 보겠습니다\n",
        "[sklearn.metrics.pairwise.cosine_similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn-metrics-pairwise-cosine-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp6Jm7UfhA1j"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# input, X:(n_samples_X, n_features)\n",
        "distance_matrix  = cosine_similarity(dtm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9c8KIXUhA1k"
      },
      "source": [
        "df = pd.DataFrame(distance_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m_so22thA1k"
      },
      "source": [
        "유사도는 문서 x 문서 행렬로 표현됩니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNP1zFs2hA1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6c1104-3c34-4dc7-894a-c78dc8cf5ad0"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 401)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMii6L8AhA1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "26a2cf07-f90b-4c1c-8ff0-f7b82c5ffdfd"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>361</th>\n",
              "      <th>362</th>\n",
              "      <th>363</th>\n",
              "      <th>364</th>\n",
              "      <th>365</th>\n",
              "      <th>366</th>\n",
              "      <th>367</th>\n",
              "      <th>368</th>\n",
              "      <th>369</th>\n",
              "      <th>370</th>\n",
              "      <th>371</th>\n",
              "      <th>372</th>\n",
              "      <th>373</th>\n",
              "      <th>374</th>\n",
              "      <th>375</th>\n",
              "      <th>376</th>\n",
              "      <th>377</th>\n",
              "      <th>378</th>\n",
              "      <th>379</th>\n",
              "      <th>380</th>\n",
              "      <th>381</th>\n",
              "      <th>382</th>\n",
              "      <th>383</th>\n",
              "      <th>384</th>\n",
              "      <th>385</th>\n",
              "      <th>386</th>\n",
              "      <th>387</th>\n",
              "      <th>388</th>\n",
              "      <th>389</th>\n",
              "      <th>390</th>\n",
              "      <th>391</th>\n",
              "      <th>392</th>\n",
              "      <th>393</th>\n",
              "      <th>394</th>\n",
              "      <th>395</th>\n",
              "      <th>396</th>\n",
              "      <th>397</th>\n",
              "      <th>398</th>\n",
              "      <th>399</th>\n",
              "      <th>400</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.014998</td>\n",
              "      <td>0.041514</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.011299</td>\n",
              "      <td>0.021492</td>\n",
              "      <td>0.008861</td>\n",
              "      <td>0.019746</td>\n",
              "      <td>0.308156</td>\n",
              "      <td>0.019956</td>\n",
              "      <td>0.027771</td>\n",
              "      <td>0.020529</td>\n",
              "      <td>0.007588</td>\n",
              "      <td>0.045374</td>\n",
              "      <td>0.019767</td>\n",
              "      <td>0.047641</td>\n",
              "      <td>0.017975</td>\n",
              "      <td>0.019942</td>\n",
              "      <td>0.018530</td>\n",
              "      <td>0.034216</td>\n",
              "      <td>0.015849</td>\n",
              "      <td>0.055570</td>\n",
              "      <td>0.021976</td>\n",
              "      <td>0.035695</td>\n",
              "      <td>0.024257</td>\n",
              "      <td>0.012487</td>\n",
              "      <td>0.009431</td>\n",
              "      <td>0.028583</td>\n",
              "      <td>0.033699</td>\n",
              "      <td>0.031327</td>\n",
              "      <td>0.041125</td>\n",
              "      <td>0.023959</td>\n",
              "      <td>0.346372</td>\n",
              "      <td>0.097859</td>\n",
              "      <td>0.012726</td>\n",
              "      <td>0.020045</td>\n",
              "      <td>0.014379</td>\n",
              "      <td>0.030776</td>\n",
              "      <td>0.372439</td>\n",
              "      <td>0.210640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050091</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.007387</td>\n",
              "      <td>0.291799</td>\n",
              "      <td>0.014685</td>\n",
              "      <td>0.031490</td>\n",
              "      <td>0.021084</td>\n",
              "      <td>0.118558</td>\n",
              "      <td>0.064996</td>\n",
              "      <td>0.021972</td>\n",
              "      <td>0.021492</td>\n",
              "      <td>0.080722</td>\n",
              "      <td>0.024713</td>\n",
              "      <td>0.024085</td>\n",
              "      <td>0.013938</td>\n",
              "      <td>0.018448</td>\n",
              "      <td>0.634388</td>\n",
              "      <td>0.030939</td>\n",
              "      <td>0.017447</td>\n",
              "      <td>0.014625</td>\n",
              "      <td>0.021464</td>\n",
              "      <td>0.038563</td>\n",
              "      <td>0.008291</td>\n",
              "      <td>0.052448</td>\n",
              "      <td>0.041029</td>\n",
              "      <td>0.026842</td>\n",
              "      <td>0.025648</td>\n",
              "      <td>0.005956</td>\n",
              "      <td>0.040467</td>\n",
              "      <td>0.034726</td>\n",
              "      <td>0.162121</td>\n",
              "      <td>0.229296</td>\n",
              "      <td>0.011844</td>\n",
              "      <td>0.018227</td>\n",
              "      <td>0.028107</td>\n",
              "      <td>0.011830</td>\n",
              "      <td>0.019738</td>\n",
              "      <td>0.013596</td>\n",
              "      <td>0.009184</td>\n",
              "      <td>0.052448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.014998</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.024811</td>\n",
              "      <td>0.036935</td>\n",
              "      <td>0.029222</td>\n",
              "      <td>0.221671</td>\n",
              "      <td>0.031656</td>\n",
              "      <td>0.036121</td>\n",
              "      <td>0.025296</td>\n",
              "      <td>0.043338</td>\n",
              "      <td>0.028627</td>\n",
              "      <td>0.040830</td>\n",
              "      <td>0.039587</td>\n",
              "      <td>0.039457</td>\n",
              "      <td>0.041103</td>\n",
              "      <td>0.022156</td>\n",
              "      <td>0.024584</td>\n",
              "      <td>0.036916</td>\n",
              "      <td>0.018690</td>\n",
              "      <td>0.085091</td>\n",
              "      <td>0.012136</td>\n",
              "      <td>0.016791</td>\n",
              "      <td>0.054708</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.028373</td>\n",
              "      <td>0.190720</td>\n",
              "      <td>0.112254</td>\n",
              "      <td>0.009806</td>\n",
              "      <td>0.014573</td>\n",
              "      <td>0.021385</td>\n",
              "      <td>0.040554</td>\n",
              "      <td>0.054340</td>\n",
              "      <td>0.017043</td>\n",
              "      <td>0.037784</td>\n",
              "      <td>0.033729</td>\n",
              "      <td>0.032849</td>\n",
              "      <td>0.040577</td>\n",
              "      <td>0.077081</td>\n",
              "      <td>0.032485</td>\n",
              "      <td>0.028017</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047781</td>\n",
              "      <td>0.036935</td>\n",
              "      <td>0.066005</td>\n",
              "      <td>0.054850</td>\n",
              "      <td>0.017615</td>\n",
              "      <td>0.036496</td>\n",
              "      <td>0.023888</td>\n",
              "      <td>0.055365</td>\n",
              "      <td>0.031680</td>\n",
              "      <td>0.057967</td>\n",
              "      <td>0.221671</td>\n",
              "      <td>0.068739</td>\n",
              "      <td>0.078008</td>\n",
              "      <td>0.025764</td>\n",
              "      <td>0.077448</td>\n",
              "      <td>0.037202</td>\n",
              "      <td>0.025191</td>\n",
              "      <td>0.020430</td>\n",
              "      <td>0.050165</td>\n",
              "      <td>0.049785</td>\n",
              "      <td>0.044212</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.032681</td>\n",
              "      <td>0.022416</td>\n",
              "      <td>0.024362</td>\n",
              "      <td>0.037098</td>\n",
              "      <td>0.017768</td>\n",
              "      <td>0.038760</td>\n",
              "      <td>0.022998</td>\n",
              "      <td>0.019019</td>\n",
              "      <td>0.015845</td>\n",
              "      <td>0.012626</td>\n",
              "      <td>0.022334</td>\n",
              "      <td>0.019251</td>\n",
              "      <td>0.042788</td>\n",
              "      <td>0.016940</td>\n",
              "      <td>0.052463</td>\n",
              "      <td>0.032662</td>\n",
              "      <td>0.037820</td>\n",
              "      <td>0.022416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.041514</td>\n",
              "      <td>0.024811</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.026519</td>\n",
              "      <td>0.102332</td>\n",
              "      <td>0.030127</td>\n",
              "      <td>0.022638</td>\n",
              "      <td>0.019852</td>\n",
              "      <td>0.040847</td>\n",
              "      <td>0.024582</td>\n",
              "      <td>0.021479</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.020282</td>\n",
              "      <td>0.044144</td>\n",
              "      <td>0.017505</td>\n",
              "      <td>0.032934</td>\n",
              "      <td>0.010031</td>\n",
              "      <td>0.034399</td>\n",
              "      <td>0.056063</td>\n",
              "      <td>0.050406</td>\n",
              "      <td>0.009653</td>\n",
              "      <td>0.013851</td>\n",
              "      <td>0.053768</td>\n",
              "      <td>0.027988</td>\n",
              "      <td>0.043993</td>\n",
              "      <td>0.039948</td>\n",
              "      <td>0.026018</td>\n",
              "      <td>0.010103</td>\n",
              "      <td>0.016034</td>\n",
              "      <td>0.027391</td>\n",
              "      <td>0.305195</td>\n",
              "      <td>0.009918</td>\n",
              "      <td>0.035028</td>\n",
              "      <td>0.036705</td>\n",
              "      <td>0.014177</td>\n",
              "      <td>0.027256</td>\n",
              "      <td>0.047349</td>\n",
              "      <td>0.033202</td>\n",
              "      <td>0.042810</td>\n",
              "      <td>0.051493</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056372</td>\n",
              "      <td>0.026519</td>\n",
              "      <td>0.001741</td>\n",
              "      <td>0.022204</td>\n",
              "      <td>0.010895</td>\n",
              "      <td>0.039594</td>\n",
              "      <td>0.026958</td>\n",
              "      <td>0.025563</td>\n",
              "      <td>0.049380</td>\n",
              "      <td>0.068558</td>\n",
              "      <td>0.030127</td>\n",
              "      <td>0.063957</td>\n",
              "      <td>0.027046</td>\n",
              "      <td>0.130258</td>\n",
              "      <td>0.008647</td>\n",
              "      <td>0.086470</td>\n",
              "      <td>0.038239</td>\n",
              "      <td>0.018313</td>\n",
              "      <td>0.030108</td>\n",
              "      <td>0.038741</td>\n",
              "      <td>0.009442</td>\n",
              "      <td>0.015001</td>\n",
              "      <td>0.018815</td>\n",
              "      <td>0.025128</td>\n",
              "      <td>0.037379</td>\n",
              "      <td>0.039582</td>\n",
              "      <td>0.014557</td>\n",
              "      <td>0.018246</td>\n",
              "      <td>0.024711</td>\n",
              "      <td>0.094204</td>\n",
              "      <td>0.006628</td>\n",
              "      <td>0.040698</td>\n",
              "      <td>0.008616</td>\n",
              "      <td>0.019255</td>\n",
              "      <td>0.021837</td>\n",
              "      <td>0.097403</td>\n",
              "      <td>0.033770</td>\n",
              "      <td>0.058408</td>\n",
              "      <td>0.055404</td>\n",
              "      <td>0.025128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.036935</td>\n",
              "      <td>0.026519</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.033863</td>\n",
              "      <td>0.035101</td>\n",
              "      <td>0.024513</td>\n",
              "      <td>0.044580</td>\n",
              "      <td>0.056627</td>\n",
              "      <td>0.041961</td>\n",
              "      <td>0.027201</td>\n",
              "      <td>0.028365</td>\n",
              "      <td>0.025973</td>\n",
              "      <td>0.047236</td>\n",
              "      <td>0.035606</td>\n",
              "      <td>0.036020</td>\n",
              "      <td>0.028055</td>\n",
              "      <td>0.027350</td>\n",
              "      <td>0.012107</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.130271</td>\n",
              "      <td>0.036489</td>\n",
              "      <td>0.045174</td>\n",
              "      <td>0.073751</td>\n",
              "      <td>0.070594</td>\n",
              "      <td>0.056464</td>\n",
              "      <td>0.048537</td>\n",
              "      <td>0.026928</td>\n",
              "      <td>0.010123</td>\n",
              "      <td>0.017588</td>\n",
              "      <td>0.074734</td>\n",
              "      <td>0.022392</td>\n",
              "      <td>0.014626</td>\n",
              "      <td>0.085479</td>\n",
              "      <td>0.023353</td>\n",
              "      <td>0.036605</td>\n",
              "      <td>0.043523</td>\n",
              "      <td>0.071219</td>\n",
              "      <td>0.054669</td>\n",
              "      <td>0.030636</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088916</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.024633</td>\n",
              "      <td>0.064317</td>\n",
              "      <td>0.026145</td>\n",
              "      <td>0.080492</td>\n",
              "      <td>0.032622</td>\n",
              "      <td>0.061752</td>\n",
              "      <td>0.078756</td>\n",
              "      <td>0.058491</td>\n",
              "      <td>0.035101</td>\n",
              "      <td>0.030452</td>\n",
              "      <td>0.054023</td>\n",
              "      <td>0.023876</td>\n",
              "      <td>0.034721</td>\n",
              "      <td>0.037571</td>\n",
              "      <td>0.021993</td>\n",
              "      <td>0.419053</td>\n",
              "      <td>0.039765</td>\n",
              "      <td>0.033855</td>\n",
              "      <td>0.049457</td>\n",
              "      <td>0.096033</td>\n",
              "      <td>0.011827</td>\n",
              "      <td>0.044153</td>\n",
              "      <td>0.028183</td>\n",
              "      <td>0.046056</td>\n",
              "      <td>0.029091</td>\n",
              "      <td>0.019028</td>\n",
              "      <td>0.288912</td>\n",
              "      <td>0.023389</td>\n",
              "      <td>0.054317</td>\n",
              "      <td>0.050017</td>\n",
              "      <td>0.029934</td>\n",
              "      <td>0.021308</td>\n",
              "      <td>0.052214</td>\n",
              "      <td>0.017941</td>\n",
              "      <td>0.028565</td>\n",
              "      <td>0.032798</td>\n",
              "      <td>0.035675</td>\n",
              "      <td>0.044153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.011299</td>\n",
              "      <td>0.029222</td>\n",
              "      <td>0.102332</td>\n",
              "      <td>0.033863</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.015927</td>\n",
              "      <td>0.021267</td>\n",
              "      <td>0.042578</td>\n",
              "      <td>0.017994</td>\n",
              "      <td>0.037723</td>\n",
              "      <td>0.019082</td>\n",
              "      <td>0.037993</td>\n",
              "      <td>0.024628</td>\n",
              "      <td>0.025644</td>\n",
              "      <td>0.026537</td>\n",
              "      <td>0.044846</td>\n",
              "      <td>0.018199</td>\n",
              "      <td>0.040985</td>\n",
              "      <td>0.214319</td>\n",
              "      <td>0.043961</td>\n",
              "      <td>0.029113</td>\n",
              "      <td>0.004304</td>\n",
              "      <td>0.070805</td>\n",
              "      <td>0.046170</td>\n",
              "      <td>0.013404</td>\n",
              "      <td>0.030189</td>\n",
              "      <td>0.035723</td>\n",
              "      <td>0.034755</td>\n",
              "      <td>0.040293</td>\n",
              "      <td>0.025615</td>\n",
              "      <td>0.023515</td>\n",
              "      <td>0.022184</td>\n",
              "      <td>0.008476</td>\n",
              "      <td>0.032747</td>\n",
              "      <td>0.014660</td>\n",
              "      <td>0.014011</td>\n",
              "      <td>0.183162</td>\n",
              "      <td>0.079023</td>\n",
              "      <td>0.075975</td>\n",
              "      <td>0.020922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046772</td>\n",
              "      <td>0.033863</td>\n",
              "      <td>0.022562</td>\n",
              "      <td>0.041920</td>\n",
              "      <td>0.017766</td>\n",
              "      <td>0.023313</td>\n",
              "      <td>0.013541</td>\n",
              "      <td>0.068659</td>\n",
              "      <td>0.022806</td>\n",
              "      <td>0.235065</td>\n",
              "      <td>0.015927</td>\n",
              "      <td>0.215408</td>\n",
              "      <td>0.066968</td>\n",
              "      <td>0.367712</td>\n",
              "      <td>0.011235</td>\n",
              "      <td>0.040682</td>\n",
              "      <td>0.010589</td>\n",
              "      <td>0.042264</td>\n",
              "      <td>0.067912</td>\n",
              "      <td>0.051215</td>\n",
              "      <td>0.055502</td>\n",
              "      <td>0.032859</td>\n",
              "      <td>0.024812</td>\n",
              "      <td>0.010446</td>\n",
              "      <td>0.098037</td>\n",
              "      <td>0.100497</td>\n",
              "      <td>0.051738</td>\n",
              "      <td>0.036786</td>\n",
              "      <td>0.026930</td>\n",
              "      <td>0.186684</td>\n",
              "      <td>0.036569</td>\n",
              "      <td>0.010800</td>\n",
              "      <td>0.018684</td>\n",
              "      <td>0.021447</td>\n",
              "      <td>0.054540</td>\n",
              "      <td>0.327685</td>\n",
              "      <td>0.040275</td>\n",
              "      <td>0.174066</td>\n",
              "      <td>0.245809</td>\n",
              "      <td>0.010446</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 401 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       398       399       400\n",
              "0  1.000000  0.014998  0.041514  ...  0.013596  0.009184  0.052448\n",
              "1  0.014998  1.000000  0.024811  ...  0.032662  0.037820  0.022416\n",
              "2  0.041514  0.024811  1.000000  ...  0.058408  0.055404  0.025128\n",
              "3  0.035916  0.036935  0.026519  ...  0.032798  0.035675  0.044153\n",
              "4  0.011299  0.029222  0.102332  ...  0.174066  0.245809  0.010446\n",
              "\n",
              "[5 rows x 401 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SWlGndPhA1k"
      },
      "source": [
        "#### 문서간 유사도 측정\n",
        "문서0과 문서0은 같으므로 유사도가 1입니다. 문서0과 문서1~4 와의의 유사도를 확인해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnyM8LnehA1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1bdaaf-7f7f-4f5a-f936-9787ebc50a08"
      },
      "source": [
        "df[0][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1.000000\n",
              "1    0.014998\n",
              "2    0.041514\n",
              "3    0.035916\n",
              "4    0.011299\n",
              "Name: 0, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7x3f9d1hA1l"
      },
      "source": [
        "#### 유사도를 이용한 정렬\n",
        "\n",
        "문서0과 유사도가 큰 문서를 순서대로 정렬해서 살펴보겠습니다 (TF-IDF vectorization 방법에 따라 많이 다를 수 있습니다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB2EiAUChA1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00cba837-067c-434d-ba41-95a28012676e"
      },
      "source": [
        "ind = df[df[0] < 1][0].sort_values(ascending=False)[:5]\n",
        "ind"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "377    0.634388\n",
              "301    0.634388\n",
              "222    0.595379\n",
              "38     0.372439\n",
              "32     0.346372\n",
              "Name: 0, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh0LsGb4dwgZ"
      },
      "source": [
        "index = 377"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUk_QuWNhA1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62eb7fc4-b698-44e7-ed22-687d2f5e7407"
      },
      "source": [
        "print(data[0][:100])\n",
        "print(data[index][:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Joke e-mail virus tricks users\\n\\nA virus that disguises itself as a joke is spreading rapidly across '\n",
            "b'Virus poses as Christmas e-mail\\n\\nSecurity firms are warning about a Windows virus disguising itself '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XI2RDt4hA1l"
      },
      "source": [
        "코사인 유사도와 같은 Brute Force 방법은 비교해야 할 문서의 양이 많아 질 수록 많은 계산을 필요로합니다. <br> 실제 어플리케이션 환경에서는 더 빠른 비교 방법을 사용해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmTXnGIChA1l"
      },
      "source": [
        "### NearestNeighbor (K-NN, K-최근접 이웃) \n",
        "\n",
        "K-최근접 이웃법은 쿼리와 가장 가까운 상위 K개의 근접한 데이터를 찾아서 K개 데이터의 유사성을 기반으로 **점을 추정하거나 분류**하는 예측 분석에 사용됩니다.\n",
        "최근접 이웃 방법은 non-generalizing 머신러닝 방법인데, 모든 학습 데이터를 KD Tree 나 Ball Tree같은 빠른 색인 구조(indexing structure)에 단순히 저장하기 때문입니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64mTWdKZpK-b"
      },
      "source": [
        "[K-D Tree](https://scikit-learn.org/stable/modules/neighbors.html?highlight=very%20distant%20from%20point#k-d-tree)의 기본아이디어는 점 A와 B가 멀고, B가 C와 가까우면 A가 C와 멀다는 것을 알 수 있는데 이때 명시적으로 A와 C의 거리를 계산할 필요가 없다는 것입니다.\n",
        "\n",
        "<img src=\"https://i.imgur.com/CKKoz5W.png\" width=\"500\"/>\n",
        "\n",
        "[Ball Tree](https://scikit-learn.org/stable/modules/neighbors.html?highlight=very%20distant%20from%20point#ball-tree)는 K-D 트리를 더욱 효율적으로 만들기 위해 개발되었습니다. KD 트리는 데이터를 Cartesian 축으로 분할하지만 Ball Tree는 nesting hyper-spheres 형태로 분할하여 트리구성에 비용이 더 들지만, 매우 구조화된 데이터나 높은 차원의 데이터에 더 효율적입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhso8V_WhA1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "6d0fe15f-2778-41f0-842a-68e0142dce50"
      },
      "source": [
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-PRON-</th>\n",
              "      <th>abandon</th>\n",
              "      <th>ability</th>\n",
              "      <th>ability record</th>\n",
              "      <th>able</th>\n",
              "      <th>able access</th>\n",
              "      <th>able choose</th>\n",
              "      <th>able control</th>\n",
              "      <th>able handle</th>\n",
              "      <th>able offer</th>\n",
              "      <th>able play</th>\n",
              "      <th>able store</th>\n",
              "      <th>able thing</th>\n",
              "      <th>able use</th>\n",
              "      <th>able watch</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absorb</th>\n",
              "      <th>abuse</th>\n",
              "      <th>academic</th>\n",
              "      <th>academy</th>\n",
              "      <th>accept</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>acceptance</th>\n",
              "      <th>access</th>\n",
              "      <th>access available</th>\n",
              "      <th>access datum</th>\n",
              "      <th>access device</th>\n",
              "      <th>access e</th>\n",
              "      <th>access grow</th>\n",
              "      <th>access home</th>\n",
              "      <th>access information</th>\n",
              "      <th>access internet</th>\n",
              "      <th>access medium</th>\n",
              "      <th>access mobile</th>\n",
              "      <th>access net</th>\n",
              "      <th>access point</th>\n",
              "      <th>access service</th>\n",
              "      <th>accessible</th>\n",
              "      <th>accident</th>\n",
              "      <th>acclaim</th>\n",
              "      <th>...</th>\n",
              "      <th>year launch</th>\n",
              "      <th>year microsoft</th>\n",
              "      <th>year million</th>\n",
              "      <th>year motorola</th>\n",
              "      <th>year new</th>\n",
              "      <th>year number</th>\n",
              "      <th>year old</th>\n",
              "      <th>year people</th>\n",
              "      <th>year portable</th>\n",
              "      <th>year predict</th>\n",
              "      <th>year real</th>\n",
              "      <th>year release</th>\n",
              "      <th>year report</th>\n",
              "      <th>year say</th>\n",
              "      <th>year service</th>\n",
              "      <th>year think</th>\n",
              "      <th>year time</th>\n",
              "      <th>year uk</th>\n",
              "      <th>year use</th>\n",
              "      <th>year year</th>\n",
              "      <th>yen</th>\n",
              "      <th>yepp</th>\n",
              "      <th>yes</th>\n",
              "      <th>york</th>\n",
              "      <th>york base</th>\n",
              "      <th>york state</th>\n",
              "      <th>york times</th>\n",
              "      <th>young</th>\n",
              "      <th>young americans</th>\n",
              "      <th>young people</th>\n",
              "      <th>young user</th>\n",
              "      <th>youngster</th>\n",
              "      <th>youth</th>\n",
              "      <th>zelda</th>\n",
              "      <th>zen</th>\n",
              "      <th>zen micro</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombie bot</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.068952</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.074656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.052588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.049556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7604 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   -PRON-  abandon   ability  ability record  ...  zombie  zombie bot  zone  zoom\n",
              "0     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "1     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "2     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "3     0.0      0.0  0.024268             0.0  ...     0.0         0.0   0.0   0.0\n",
              "4     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 7604 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBToNaAhhA1m"
      },
      "source": [
        "#### sklearn - NearestNeighbors\n",
        "sklearn에서 비지도학습을 위한 NearestNeighbors 모델을 사용합니다\n",
        "[sklearn.neighbors.NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn-neighbors-nearestneighbors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7z2v2aHhA1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0817e9a1-a9b3-483b-c4c0-1f44638f7a0f"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# dtm을 사용히 NN 모델을 학습시킵니다. (디폴트)최근접 5 이웃.\n",
        "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
        "nn.fit(dtm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrmaUZU5hA1m"
      },
      "source": [
        "문서0과 가장 가까운 문서 (0 포함) 5개의 거리(값이 작을수록 유사합니다)와, 문서의 인덱스를 알 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8VirpVdhA1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed34f898-b14b-4667-8a7f-02dab9940626"
      },
      "source": [
        "nn.kneighbors([dtm.iloc[0].values])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.85511637, 0.85511637, 0.89957864, 1.12032242]]),\n",
              " array([[  0, 377, 301, 222,  38]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCOAsaBRhA1m"
      },
      "source": [
        "문서0의 이웃인 문서377로 검색해 보겠습니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mQaJkM4hA1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35b8a44-f57b-4a2d-be62-85b3c7581d05"
      },
      "source": [
        "nn.kneighbors([dtm.iloc[377]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.        , 0.85511637, 0.91629024, 1.10942847]]),\n",
              " array([[377, 301,   0, 222,  38]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uUn6P5OhA1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e777f87-9018-47e9-9634-aa9e77b0194b"
      },
      "source": [
        "print(data[222][:300])\n",
        "print(data[301][:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Toxic web links help virus spread\\n\\nVirus writers have begun using the power of the web to spread their malicious wares.\\n\\nA Windows virus called Bofra is turning infected machines into distributors of its malicious code. Those clicking on the poisoned links in e-mail messages sent out by infected mac'\n",
            "b'Virus poses as Christmas e-mail\\n\\nSecurity firms are warning about a Windows virus disguising itself as an electronic Christmas card.\\n\\nThe Zafi.D virus translates the Christmas greeting on its subject line into the language of the person receiving infected e-mail. Anti-virus firms speculate that this'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ7KPCgJhA1n"
      },
      "source": [
        "#### 문서 검색 예제\n",
        "\n",
        "CNN 에서 tech 기사를 가져와서 문서검색에 사용해봅시다. 다른 기사를 가지고 테스트해봐도 좋습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gh7h7IlhA1n"
      },
      "source": [
        "# https://edition.cnn.com/2020/07/30/tech/huawei-samsung-q2-hnk-intl/index.html\n",
        "cnn_tech_article = [ \"\"\"\n",
        "Hong Kong (CNN Business)Huawei became the world's top smartphone seller last quarter, overtaking Samsung for the first time ever, according to an independent market research report released Thursday.\n",
        "The Chinese tech company shipped 55.8 million phones in the three months ended in June, surpassing longtime rival Samsung, which shipped 53.7 million, according to the Canalys report.\n",
        "\"Taking first place is very important for Huawei,\" said Canalys analyst Mo Jia. \"It is desperate to showcase its brand strength to domestic consumers, component suppliers and developers.\"\n",
        "A years-long US pressure campaign against Huawei has handicapped the Shenzhen-based firm's global business.\n",
        "Huawei still suffered an annual decline in smartphone shipments of 5%. But Samsung's was a lot bigger at 30%, according to Canalys.\n",
        "The market research firm said Huawei's victory over Samsung wouldn't have happened without Covid-19. The company was able to take advantage of the economic recovery in China, where Huawei now sells over 70% of its smartphones. Samsung has a very small presence in China.\n",
        "Huawei&#39;s hopes of global domination have been dashed\n",
        "Huawei's hopes of global domination have been dashed\n",
        "Huawei's global smartphone and telecom gear business continues to suffer the fallout from US sanctions that cut the company off from key American tech and supplies.\n",
        "Without access to popular Google (GOOGL GOOGLE) apps such as YouTube, maps and Gmail, Huawei's latest smartphones are a lot less attractive to international buyers. That will make it very difficult for Huawei to hold on to the global No. 1 position, according to Jia.\n",
        "\"It will be hard for Huawei to maintain its lead in the long term. Its major channel partners in key regions, such as Europe, are increasingly wary of ranging Huawei devices, taking on fewer models, and bringing in new brands to reduce risk. Strength in China alone will not be enough to sustain Huawei at the top once the global economy starts to recover,\" he said.\n",
        "\"Our business has demonstrated exceptional resilience in these difficult times,\" Huawei spokeswoman Evita Cao said. Cao did not respond to questions on how the company can maintain its lead going forward.\n",
        "Huawei's victory came on the same day Samsung posted a big profit bump for the second quarter, with strong chip demand helping the company weather the fallout from the coronavirus pandemic.\n",
        "Samsung reported operating profit of 8.15 trillion won ($6.8 billion) for the three months that ended in June, up more than 23% compared to the same period last year.\n",
        "Samsung said sales fell about 6% to 53 trillion won ($44.6 billion).\n",
        "Shares in Samsung were last up 0.7% in Seoul. South Korea's Kospi (KOSPI) rose 0.1%.\n",
        "Taiwan&#39;s TSMC is becoming one of the world&#39;s top companies. Intel&#39;s problems are helping\n",
        "Taiwan's TSMC is becoming one of the world's top companies. Intel's problems are helping\n",
        "Despite the double digit declines in annual smartphone shipments for the quarter noted by the Canalys report, Samsung reported that the unit remained profitable thanks to savings on marketing costs. (Samsung does not break out specifics about its smartphone shipments, but noted that they declined.)\n",
        "For the second half of 2020, however, Samsung is warning that \"uncertainties related to Covid-19 linger\" for its mobile business.\n",
        "That could be enough to drag the company to revenue losses for the year, according to research firm Crisp Idea.\n",
        "The consumer electronics unit, which includes smartphones and TVs, is \"expected to decline significantly as Covid-19 affects demand and leads to store and plant closures globally,\" Crisp Idea analysts wrote in a note earlier this month.\n",
        "Smartphone shipments worldwide are expected to fall about 18% in the first half of the year as the pandemic continues to affect consumer spending, analysts at IDC said in a note last month.\n",
        "The market research firm added that global smartphone shipments are not expected to return to growth until the first quarter of 2021.\n",
        "That would also hurt Samsung's memory chip business, because the company supplies chips for rival smartphone companies such as Apple (AAPL) and Huawei.\"\"\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vOr8kVjhA1n"
      },
      "source": [
        "CNN Tech 뉴스를 쿼리로 쓰기 위해 학습된 tfidf vectorizer를 통해 변환하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsCn9IkPhA1n"
      },
      "source": [
        "new = tfidf.transform(cnn_tech_article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pVXr1YzhA1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485e7b90-b601-4f83-bd01-c0a7b4aa128e"
      },
      "source": [
        "nn.kneighbors(new.todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.31624638, 1.3174223 , 1.31770469, 1.31778057, 1.31848904]]),\n",
              " array([[297, 218,  56, 374, 232]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iSQ0s5-hA1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac271f7-625c-4bcf-ea4f-2731d78dca2d"
      },
      "source": [
        "# 가장 가깝게 나온 문서를 확인합니다 \n",
        "data[297]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Gadget growth fuels eco concerns\\n\\nTechnology firms and gadget lovers are being urged to think more about the environment when buying and disposing of the latest hi-tech products.\\n\\nAt the Consumer Electronics Show in Las Vegas earlier this month, several hi-tech firms were recognised for their strategies to help the environment. Ebay also announced the Rethink project bringing together Intel, Apple, and IBM among others to promote recycling. The US consumer electronics market is set to grow by over 11% in 2005. But more awareness is needed about how and where old gadgets can be recycled as well as how to be more energy efficient, said the US Environmental Protection Agency (EPA). Of particular growing concern is how much energy it takes to recharge portable devices, one of the fastest growing markets in technology. The Consumer Electronics Association (CEA) has predicted that shipments of consumer technologies in 2005 will reach more than $125.73 billion (nearly \\xc2\\xa368 billion).\\n\\nEbay\\'s initiative pulls together major technology firms, environment groups, government agencies and eBay users to give information about what to do with old computers and where to send them. The online auction house thinks that its already-established community of loyal users could be influential. \"We really became aware of the e-waste issue and we saw that our 125 million users can be a powerful force for good,\" eBay\\'s David Stern told the BBC News website.\\n\\n\"We saw the opportunity to meet the additional demand we have on the site for used computers and saw the opportunity too to good some good for the environment.\" But it is not just computers that cause a problem for the environment. Teenagers get a new mobile every 11 months, adults every 18 months and a 15 million handsets are replaced in total each year. Yet, only 15% are actually recycled. This year, a predicted two billion people worldwide will own a mobile, according to a Deloitte report. Schemes in the US, like RIPMobile, could help in targeting younger generations with recycling messages. The initiative, which was also launched at CES, rewards 10 to 28-year-olds for returning unused phones. \"This system allows for the transformation of a drawer full of unused mobile phones into anything from music to clothes to electronics or games,\" said Seth Heine from RIPMobile.\\n\\nOne group of students collected 1,000 mobiles for recycling in just three months. Mr Heine told the BBC News website that what was important was to raise awareness amongst the young so that recycling becomes \"learned behaviour\". Europe is undoubtedly more advanced than the US in terms of recycling awareness and robust \"end of life\" programmes, although there is a tide change happening in the rest of the world too. Intel showcased some its motherboards and chips at CES which are entirely lead free.\\n\\n\"There is more and more awareness on the consumer side, but the whole industry is moving towards being lead free,\" Intel\\'s Allen Wilson told the BBC News website. \"There is still low-level awareness right now, but it is on the rise - the highest level of awareness is in Europe.\" A European Union (EU) directive, WEEE (Waste Electronic and Electrical Equipment), comes into effect in August. It puts the responsibility on electrical manufacturers to recycle items that are returned to them. But developments are also being made to design better technologies which are more energy efficient and which do not contain harmful substances. Elements like chromium, lead, and cadmium - common in consumer electronics goods - will be prohibited in all products in the EU by 2006.\\n\\nBut it is not just about recycling either. The predicted huge growth in the gadget market means the amount of energy used to power them up is on the rise too. The biggest culprit, according to the EPA, is the innocuous power adaptor, nicknamed \"energy vampires\". They provide vital juice for billions of mobile phones, PDAs (personal digital assistants), digital cameras, camcorders, and digital music players.\\n\\nAlthough there is a focus on developing efficient and improved circuits in the devices themselves, the technologies inside rechargers are still outdated and so eat up more energy than is needed to power a gadget. On 1 January, new efficiency standards for external power supplies came into effect as part of the European Commission Code of Conduct. But at CES, the EPA also unveiled new guidelines for its latest Energy Star initiative which targets external power adapters. These map out the framework for developing better adaptors that can be labelled with an Energy Star logo, meaning they are about 35% more efficient. The initiative is a global effort and more manufacturers\\' adaptors are being brought on board. Most are made in China. About two billion are shipped global every year, and about three billion are in use in the US alone. The EPA is already working with several companies which make more than 22% of power supplies on the market. \"We are increasingly finding companies that not only want to provide neat, hi-tech devices, but also bundle with it a hi-tech, efficient power supply,\" the EPA\\'s Andrew Fanara said. Initiatives like this are critical; if power adaptors continue to be made and used as they are now, consumer electronics and other small appliances will be responsible for more than 40% of electricity used in US homes, said the EPA.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUuQQBwFcw0Z"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRmbTtyecw0Z"
      },
      "source": [
        "여러분은 이미 머신러닝을 이용해 분류기를 학습시킬 수 있습니다. 그리고 텍스트 문서에서 어떻게 특성들을 추출하는지 배웠습니다. 이제 텍스트 문서를 분류하는 모델을 만들 차례 입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rll8BiMGcw0a"
      },
      "source": [
        "## ⛏ 텍스트에서 특성들을 추출하고 문서 분류기를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQTs6vWEcw0a"
      },
      "source": [
        "Sklearn 파이프라인을 사용하면 머신러닝 프로세스에 사용되는 여러 컴포넌트들을 쉽게 연결할 수 있었습니다.\n",
        "\n",
        "이번에는 파이프라인을 이용해 코퍼스 입력, 차원 축소, 학습 프로세스를 진행해 보겠습니다.\n",
        "\n",
        "벡터화 과정중에 n-gram 범위, 최대 토큰의 수 같은 하이퍼파라미터들을 수정해 가며 실험을 해 보아야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrJ9-RkBcw0a"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFH03a_Xcw0b"
      },
      "source": [
        "20개 뉴스그룹으로 분류된 18,000개의 뉴스그룹 문서 데이터셋 입니다.\n",
        "- [20newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)\n",
        "- 전자와 정치에 관한 두 개의 다른 카테고리 뉴스를 가져오겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB_MPmeacw0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd53800-563c-41f0-f3af-7ed6742facf6"
      },
      "source": [
        "categories = ['sci.electronics',\n",
        "              'talk.politics.misc']\n",
        "\n",
        "ng_train = fetch_20newsgroups(subset='train'\n",
        "                             , remove=('headers', 'footers', 'quotes')\n",
        "                             , categories=categories\n",
        "                             )\n",
        "\n",
        "ng_test = fetch_20newsgroups(subset='test'\n",
        "                             , remove=('headers', 'footers', 'quotes')\n",
        "                             , categories=categories\n",
        "                             )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU0Jt2iNcw0b"
      },
      "source": [
        "학습, 테스트 데이터가 분리되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYp4f-SScw0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1456095-0599-4365-dd55-a863b9ae750c"
      },
      "source": [
        "len(ng_train.data), len(ng_test.data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1056, 703)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URUyGDqvcw0c"
      },
      "source": [
        "한 문서를 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxl1DlbAcw0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "c37c9f83-f787-4649-b75e-a6b10281e17a"
      },
      "source": [
        "ng_train.data[5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nMight be a good idea...  The resolution you requested is about 0.3mV\\nIn order to get what you've paid for, noise level better be lower than\\nthat.  It is kind of hard to do it in a noisy box like you can expect\\ninside a PC.\\n\\nBefore you pay $$$ for a PC card, test it out by sampling a low\\ndistortion sine wave (I think there is a sine wave on a CD.  Digital\\nDomain ?  There are possibly other low THD sources)  Run the digitized\\nwaveform through a FFT transform and take alook at the noise floor on\\nthe spectrum.  That's should give you a good indication of the design.\\n(That's what I am doing to test a data acquistion system I have designed\\n- I got the idea from MAXIM data sheet.)\\n\\nIf you can live with 14 bit resolution, I would recommend looking at\\nthe MAX121 from MAXIM.  It is a high speed (308KHz) complete\\nsampling A/D with DSP interface.  The input range is +/- 5V and it\\nuses a serial interface (which can easily be optically isolated\\nfrom the computer to elinimate a major noise source)  The Analog design\\nguide I got from them shows a -100db noise level.  They claim a -77db\\nmax (-85 typ.) THD.  Looks pretty good for the $12 @ 1000 pieces\\n\\nA evaluation kit is available.  Might want to give these nice folks a\\ncall.  1-800-998-8800 or fax: (408)737-7194 and (408) 737-7600 ext4000\\nfor application assistance.\\n\\nThis assumes that you can build your own DAS and write your own software.\\n(Hey you can get the MAX121 as a free sample just by calling the 1-800 #)\\n\\n\\nI would recommend you to find out the resolution that can be gotten out\\nof your system by looking at the noise level, otherwise you might be\\nthrowing out your money.\\n\\n\\nK. C. Lee\\nElec. Eng. Grad. Student\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL7Bkvbrcw0c"
      },
      "source": [
        "이 문서의 타겟 레이블 입니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJYcdu4Icw0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98eb4b44-c656-490b-b544-23cac43cdaee"
      },
      "source": [
        "ng_train.target[5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6QaGCSjcw0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c30fabe-683a-4bc6-ad59-ec6a3e04c06c"
      },
      "source": [
        "ng_train.target_names"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sci.electronics', 'talk.politics.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ8Qfh_ccw0d"
      },
      "source": [
        "### 데이터를 살펴봅시다\n",
        "- [Step 1: Gather Data](https://developers.google.com/machine-learning/guides/text-classification/step-1)\n",
        "\n",
        "- [Step 2: Explore Your Data](https://developers.google.com/machine-learning/guides/text-classification/step-2)\n",
        "\n",
        "학습 모델을 만드는 일은 데이터 분석 과정 중 한 부분입니다. 모델링 전 데이터의 특성을 확인하고 이해하는 과정을 통해 더욱 좋은 모델을 만들 수 있게 됩니다. 데이터를 미리 잘 살펴보면 더 적은 데이터로 더 높은 성능을 가진 모델을 만들 수도 있습니다.\n",
        "\n",
        "- [explore_data.py](https://github.com/google/eng-edu/blob/master/ml/guides/text_classification/explore_data.py) 파일은 ipynb 폴더에 다운받아 import 하여 사용하세요\n",
        "\n",
        "- [Direct Download Link](https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/etc/explore_data.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p3OiXx9r0Up",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "193aac86-515b-428b-8c97-f9d54539ad67"
      },
      "source": [
        "# for Colab User\n",
        "# Upload files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-06d3b947-3cf9-4d37-9f97-7380ddce46ee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-06d3b947-3cf9-4d37-9f97-7380ddce46ee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving explore_data.py to explore_data.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWdlj3qqcw0d"
      },
      "source": [
        "# 그냥 실핼하면 에러가 날 수 있습니다. \n",
        "# 위 'Direct Download Link'에서 explore_data를 다운받은 뒤 업로드해서 사용합니다. \n",
        "import explore_data as ed\n",
        "import seaborn as sns"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FyEX7QLcw0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87d0f7f-ebbe-44e5-a2b1-5a21aaf8a081"
      },
      "source": [
        "# Gets the median number of words per sample given corpus.\n",
        "median_words_per_sample = ed.get_num_words_per_sample(ng_train.data)\n",
        "print('Median words per sample: ', median_words_per_sample)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Median words per sample:  91.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXwjr9-Ucw0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d0d5998e-71b8-429c-ffa1-3ade3fd0f473"
      },
      "source": [
        "# 데이터 길이 확인\n",
        "ed.plot_sample_length_distribution(ng_train.data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d6pkZEQcw0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2d0f0a53-7eda-4cfd-b1d4-5ee9fc2c2153"
      },
      "source": [
        "# 전자와 정치 두 카테고리 확인\n",
        "ed.plot_class_distribution(ng_train.target)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TijdMKoocw0e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = [15, 4]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKxlSOYJcw0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "f2ba6537-90ff-4117-fb19-3350ce4d767a"
      },
      "source": [
        "# Plots the frequency distribution of n-grams.\n",
        "ed.plot_frequency_distribution_of_ngrams(ng_train.data\n",
        "                                     , ngram_range=(1, 2)\n",
        "                                     , num_ngrams=50)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mY9oiF5cw0e"
      },
      "source": [
        "### 모델은 어떻게 선택할까요? \n",
        "- [Step 2.5: Choose a Model](https://developers.google.com/machine-learning/guides/text-classification/step-2-5)\n",
        "\n",
        "앞서 간단히 살펴본 데이터의 수치를 기반해서 2.5단계에서는 어떤 분류 모델을 사용할 것인지 선택을 해 보겠습니다.\n",
        "\n",
        "다음 플로우차트에서 어떻게 분류 모델을 선택해야 하는지 구글에서 수행한 여러 실험 결과를 가지고 간략한 가이드를 제공합니다. 목표는 주어진 데이터세트에서 가능한 최선의 정확도를 낼 수 있고 동시에 학습계산량을 줄이는 것이었습니다. 최적의 방법을 찾기 위해 감성분석, 토픽 분류 등 여러 문제들에 대한 12개의 데이터 세트를 사용했으며 또한 여러 학습 모델구조를 사용해 45만번 이상의 실험을 수행하였습니다.\n",
        "\n",
        "![flowchart](https://developers.google.com/machine-learning/guides/text-classification/images/TextClassificationFlowchart.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWqvu-skcw0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce5def8a-0584-4260-de11-690c46493972"
      },
      "source": [
        "# 데이터 출처... \n",
        "# categories = ['sci.electronics',\n",
        "#               'talk.politics.misc']\n",
        "\n",
        "# ng_train = fetch_20newsgroups(subset='train'\n",
        "#                              , remove=('headers', 'footers', 'quotes')\n",
        "#                              , categories=categories\n",
        "#                              )\n",
        "\n",
        "# ng_test = fetch_20newsgroups(subset='test'\n",
        "#                              , remove=('headers', 'footers', 'quotes')\n",
        "#                              , categories=categories\n",
        "#                              )\n",
        "\n",
        "# S/W ratio를 계산해 봅시다, 구글 flowchar에 따르면,\n",
        "# S/W < 1500 일 경우 BoW 를 사용해 벡터화 하고 simple MLP 모델 or 앙상블 모델을 사용하는것을 추천하고 있습니다.\n",
        "sw_ratio = len(ng_train.data) / median_words_per_sample\n",
        "print('number of samples / median words per sample ratio: ', int(sw_ratio))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of samples / median words per sample ratio:  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp6d-P1Zcw0f"
      },
      "source": [
        "### TF-IDF 모델로 베이스라인을 만들어 봅시다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACEA2Gu1cw0f"
      },
      "source": [
        "# 파이프라인 구성 요소를 만듭니다\n",
        "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "rfc = RandomForestClassifier()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qJydSP4cw0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf09f156-fff3-41c8-f9ea-3f2f37e24a69"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# DTM을 생성합니다.\n",
        "# fit_transform : Learn vocabulary and idf, return document-term matrix.\n",
        "dtm = vect.fit_transform(ng_train.data) \n",
        "\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
        "dtm.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1056, 97372)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLfA9rMgcw0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4fee9d-1dad-459e-cba0-1d00db5c51d0"
      },
      "source": [
        "# 파이프라인을 정의합니다\n",
        "pipe = Pipeline([\n",
        "    ('vect',vect)\n",
        "    ,('clf', rfc)\n",
        "])\n",
        "pipe"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words='english', strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patte...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noL1qns7cw0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c74d5a-93af-441a-b5e6-8aa37d9232d7"
      },
      "source": [
        "# 파라미터 셋팅\n",
        "parameters = {\n",
        "    'vect__max_df': (0.7, 1.0) # document frequency(%) 높을 경우 제거\n",
        "    ,'vect__min_df': (2, 5, 10) # document frequency(횟수) 낮을 경우 제거\n",
        "    ,'vect__max_features': (5000, 20000) # 코퍼스에서 term frequency 높은 순서대로 나열하여 제한\n",
        "    ,'clf__n_estimators': (100, 500) # The number of trees in the forest.\n",
        "    ,'clf__max_depth': (10, 20, None) # The maximum depth of the tree\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(ng_train.data, ng_train.target)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   27.5s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  7.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 2),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words='english',\n",
              "                                                        strip...\n",
              "                                                               n_jobs=None,\n",
              "                                                               oob_score=False,\n",
              "                                                               random_state=None,\n",
              "                                                               verbose=0,\n",
              "                                                               warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'clf__max_depth': (10, 20, None),\n",
              "                         'clf__n_estimators': (100, 500),\n",
              "                         'vect__max_df': (0.7, 1.0),\n",
              "                         'vect__max_features': (5000, 20000),\n",
              "                         'vect__min_df': (2, 5, 10)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0EaF-A1cw0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9b5f99-72ea-411a-cc78-d502704aae65"
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9062550299561835"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sff53Hfxcw0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35dea4d-011e-4417-f488-b75344488ad6"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf__max_depth': None,\n",
              " 'clf__n_estimators': 500,\n",
              " 'vect__max_df': 0.7,\n",
              " 'vect__max_features': 20000,\n",
              " 'vect__min_df': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNNO8lvPcw0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2157c6-899b-47be-8790-e67d7f79f0b8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 테스트 데이터에 대해 정확도를 구해보겠습니다\n",
        "y_test = grid_search.predict(ng_test.data)\n",
        "accuracy_score(ng_test.target, y_test)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8947368421052632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96o5cDQuLuj6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrYyMDPwcw0g"
      },
      "source": [
        "## 잠재의미분석(Latent Semantic Analysis, LSA)\n",
        "\n",
        "\n",
        "잠재의미분석(LSA)는 Word2Vec, BoW 등의 방법론을 사용해 만든 문서-단어행렬(DTM) 데이터의 차원을 축소해 문서들에 숨어있는(latent) 의미(Topics)를 끌어내는 방법입니다.\n",
        "\n",
        "이때 차원 축소에는 Truncated SVD(특이값 분해)를 사용해 원하는 문서나, 단어의 차원을 축소합니다.\n",
        "\n",
        "물론 차원이 축소가 되더라도 기존에 문서나, 단어들 간의 거리관계는 어느정도 보존이 됩니다.\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Konstantinos_Bougiatiotis/publication/321025221/figure/fig9/AS:668660309962763@1536432449448/Singular-value-decomposition-followed-by-rank-lowering-for-latent-semantic-indexing.jpg\" alt=\"Singular value decomposition followed by rank lowering for latent semantic indexing\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4h2qAZ8cw0g"
      },
      "source": [
        "SVD를 사용해 행렬 $A$를 $U, Σ, V^{T}$ 세 행렬의 곱으로 분해(decomposion) 합니다.\n",
        "\n",
        "> $A=UΣV^{T}$\n",
        "\n",
        "여기서 $U$, $V$가 직교행렬(orthogonal matrix) 이고, $Σ$는 대각행렬로 대각성분이 특이값(singular value)입니다.\n",
        "\n",
        "$U$, $V$가 직교행렬(orthogonal matrix)일 때 $UU^T=U^TU=I$, $VV^T=V^TV=I$ 관계가 성립하므로\n",
        "\n",
        "> $AV=UΣ$  입니다\n",
        "\n",
        "$U$ 와 $V^T$ 의 열 벡터는 특이벡터(singular vector)라 불리는데\n",
        "\n",
        "Truncated SVD는 특이값(singular value, $Σ$ 대각성분) 가운데 가장 큰 k개만 남기고 해당 특이값에 대응하는 특이벡터들로 원래 행렬 A를 근사하는 방법입니다.\n",
        "\n",
        "물론 0보다 큰 특이값을 제거하면 정보의 손실이 발생하므로 적당히 필요한 차원만큼 k를 선택합니다.\n",
        "\n",
        "다음 그림에서 k값 선택에 따른 정보 손실을 확인할 수 있습니다.\n",
        "\n",
        "<img src=\"https://i.imgur.com/Dica82I.png\" width=\"600\" />\n",
        "\n",
        "\n",
        "행렬은 선형변환으로 볼 수 있으므로, $V$의 열벡터들을 $A$를 통해 선형변환하면 $Σ$의 특이값 만큼 크기가 변하면서 여전히 직교하는 $U$의 열벡터들을 얻을 수 있다는 것입니다.\n",
        "\n",
        "\n",
        "여기서 만약 m개 문서, n개 단어로 이루어진 행렬을 truncated SVD로 분해해 다음과 같은 분해를 수행 했다면 다음과 같은 근사 식을 얻을 것이며\n",
        "\n",
        "$$A_k=U_kΣ_kV^{T}_k$$\n",
        "\n",
        "$U_k$와 $V_k$를 사용해 n차원으로 표현 되었던 문서를 k차원으로, 또는 m 차원으로 표현되었던 단어를 k 차원으로 표현할 수 있게 됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tee6pqmacw0h"
      },
      "source": [
        "#### TruncatedSVD 를 사용하여 파이프라인에서 차원을 축소하고 분류문제를 풀어보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5fOjWzNcw0h"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# SVD를 사용한 차원 축소\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "vect = TfidfVectorizer(stop_words='english'\n",
        "                       , ngram_range=(1,2)\n",
        "                       , min_df=2\n",
        "                       , max_df=0.7\n",
        "                       , token_pattern=r'(?u)\\b\\w[A-Za-z]+\\b' # 영문자만 사용\n",
        "                       , max_features=10000\n",
        "                      )\n",
        "\n",
        "svd = TruncatedSVD(algorithm='randomized'\n",
        "                   , n_iter=5\n",
        "                   , random_state=2)\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=500, random_state=2)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pqw6H2scw0i"
      },
      "source": [
        "params = {\n",
        "    # 100~500 사이의 정수 크기로 차원을 줄입니다\n",
        "#     'svd__n_components': stats.randint(100, 500)\n",
        "    'svd__n_components': stats.randint(2, 3) # 문서의 차원을 2로 고정\n",
        "    \n",
        "}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uomwXfCwcw0i"
      },
      "source": [
        "# 1.Tfidf 문서 벡터화, 2. svd 차원축소, 3. 랜덤포레스트 분류기\n",
        "pipe = Pipeline([\n",
        "    ('vect', vect)\n",
        "    , ('svd', svd)\n",
        "    , ('clf', rfc)\n",
        "])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu3Di4vDcw0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1effebf4-e082-4605-bd40-a976f2b04335"
      },
      "source": [
        "# Fit\n",
        "random_search = RandomizedSearchCV(pipe,params, cv=3, n_iter=5, n_jobs=-1, verbose=1)\n",
        "random_search.fit(ng_train.data, ng_train.target)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   16.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=Pipeline(memory=None,\n",
              "                                      steps=[('vect',\n",
              "                                              TfidfVectorizer(analyzer='word',\n",
              "                                                              binary=False,\n",
              "                                                              decode_error='strict',\n",
              "                                                              dtype=<class 'numpy.float64'>,\n",
              "                                                              encoding='utf-8',\n",
              "                                                              input='content',\n",
              "                                                              lowercase=True,\n",
              "                                                              max_df=0.7,\n",
              "                                                              max_features=10000,\n",
              "                                                              min_df=2,\n",
              "                                                              ngram_range=(1,\n",
              "                                                                           2),\n",
              "                                                              norm='l2',\n",
              "                                                              preprocessor=None,\n",
              "                                                              smooth_idf=True,\n",
              "                                                              stop_words='english...\n",
              "                                                                     n_estimators=500,\n",
              "                                                                     n_jobs=None,\n",
              "                                                                     oob_score=False,\n",
              "                                                                     random_state=2,\n",
              "                                                                     verbose=0,\n",
              "                                                                     warm_start=False))],\n",
              "                                      verbose=False),\n",
              "                   iid='deprecated', n_iter=5, n_jobs=-1,\n",
              "                   param_distributions={'svd__n_components': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f436735f250>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSixIHkucw0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120007e5-98a5-4134-c195-356f68c010be"
      },
      "source": [
        "# svd__n_components: Random search에서 선택된 줄어든 차원을 확인할 수 있습니다.\n",
        "random_search.best_params_"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'svd__n_components': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jCVajTncw0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394e5fca-ed15-49a0-f8f7-d912313ba756"
      },
      "source": [
        "random_search.best_score_"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9147727272727272"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoZUxlzccw0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929543ec-2ba8-49c2-f233-656bbbfb5a45"
      },
      "source": [
        "# 테스트셋으로 정확도를 계산합니다\n",
        "y_test = random_search.predict(ng_test.data)\n",
        "accuracy_score(ng_test.target, y_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9075391180654339"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD052pBScw0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f37eef5-d2b1-48b0-a710-f6fd1ae98989"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(ng_test.target, y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       393\n",
            "           1       0.91      0.88      0.89       310\n",
            "\n",
            "    accuracy                           0.91       703\n",
            "   macro avg       0.91      0.90      0.91       703\n",
            "weighted avg       0.91      0.91      0.91       703\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ2xkx5_cw0j"
      },
      "source": [
        "#### SVD를 따로 수행해서 행렬분해가 어떻게 되는지 확인해 보겠습니다.\n",
        "\n",
        "- randomized_svd를 사용해서 $U$, $\\Sigma$, $V^T$ 행렬을 구해보겠습니다.\n",
        "- randomized_svd는 Truncated SVD에서 내부적으로 사용되는 기능입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpw-B8BVcw0j"
      },
      "source": [
        "# 학습 데이터를 TF-IDF vectorizer로 벡터화하여 사용하겠습니다.\n",
        "A = random_search.best_estimator_.named_steps['vect'].transform(ng_train.data).todense()\n",
        "X_test = random_search.best_estimator_.named_steps['vect'].transform(ng_test.data).todense()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG9MGR7scw0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545b2c3d-7756-4abc-b7f7-e7afec27d024"
      },
      "source": [
        "# DTM\n",
        "A.shape, X_test.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1056, 10000), (703, 10000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wXozeaHcw0k"
      },
      "source": [
        "# randomized_svd를 사용하여 U, S(Sigma), VT(V transposed) 행렬을 얻습니다\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, S, VT = randomized_svd(A\n",
        "                         , n_components=2 # 상위 특이값 2개를 선택합니다\n",
        "                         , n_iter=5\n",
        "                         , random_state=2)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IlvM14zcw0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14abe1d-0a8c-4811-cea2-68d24f87d2ab"
      },
      "source": [
        "U.shape, S.shape, VT.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1056, 2), (2,), (2, 10000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2YFuXCncw0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591d2616-6ae4-4eb1-bdd6-5027931aeefc"
      },
      "source": [
        "U"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.02975338,  0.02040102],\n",
              "       [ 0.0290698 ,  0.04117074],\n",
              "       [ 0.03961388, -0.04253567],\n",
              "       ...,\n",
              "       [ 0.03970262, -0.01638489],\n",
              "       [ 0.03398986,  0.03985268],\n",
              "       [ 0.01414231, -0.01261801]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBZKePKicw0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c665c34-a4c7-42c3-8309-b552e932a056"
      },
      "source": [
        "# 대각 성분(특이값)만 가져왔습니다.\n",
        "S"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.92178091, 2.59248335])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owjWO96Ocw0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041745ae-2ce8-410a-cd6a-3705f87ae44d"
      },
      "source": [
        "VT"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00103713,  0.00276685,  0.00616488, ...,  0.00451874,\n",
              "         0.00629204,  0.0035681 ],\n",
              "       [ 0.00127521,  0.00616524,  0.01354243, ...,  0.00637487,\n",
              "         0.00368002, -0.00077783]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ_qfgq0cw0l"
      },
      "source": [
        "$V_k^{T} V_k = I_k$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veG_Llbxcw0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2650a9-1cc9-4257-d7eb-17bd430cb2a2"
      },
      "source": [
        "VT @ VT.T"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.00000000e+00, -1.90819582e-17],\n",
              "       [-1.90819582e-17,  1.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxd__xuDcw0l"
      },
      "source": [
        "$A_k=U_kΣ_kV^{T}_k$ \n",
        "\n",
        "=> $A_k V_k = U_k\\Sigma_k$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXvZ6aj5cw0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e284f76e-d723-48a4-fa21-4f43675692f9"
      },
      "source": [
        "# A는 A_k는 아니지만 결과는 같습니다.\n",
        "AV = A @ VT.T\n",
        "AV"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 0.11668625,  0.0528893 ],\n",
              "        [ 0.11400537,  0.10673446],\n",
              "        [ 0.15535694, -0.11027301],\n",
              "        ...,\n",
              "        [ 0.15570497, -0.04247756],\n",
              "        [ 0.1333008 ,  0.10331742],\n",
              "        [ 0.05546305, -0.03271198]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S2Lxdn_cw0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ef4240-cb32-4055-d652-1df1b4fd07c5"
      },
      "source": [
        "US = U @ np.diag(S)\n",
        "US"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.11668625,  0.0528893 ],\n",
              "       [ 0.11400537,  0.10673446],\n",
              "       [ 0.15535694, -0.11027301],\n",
              "       ...,\n",
              "       [ 0.15570497, -0.04247756],\n",
              "       [ 0.1333008 ,  0.10331742],\n",
              "       [ 0.05546305, -0.03271198]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq3LCwEMcw0l"
      },
      "source": [
        "테스트 문서들도 $V_k$를 사용해서 문서의 차원을 축소할 수 있습니다.\n",
        "\n",
        "$XV_k$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN1HF-7Ucw0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d802cf-8c06-418c-b46e-e7aa665a8260"
      },
      "source": [
        "X_test_trans = X_test @ VT.T\n",
        "X_test_trans"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 0.03407879, -0.03776464],\n",
              "        [ 0.14614089, -0.05216358],\n",
              "        [ 0.0676408 ,  0.07027432],\n",
              "        ...,\n",
              "        [ 0.14372308,  0.06976893],\n",
              "        [ 0.09377234,  0.04664435],\n",
              "        [ 0.10378836, -0.04427015]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7VkJmJ9cw0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cac0daa-2879-419f-fe5b-4d3f639d256c"
      },
      "source": [
        "X_test_trans.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(703, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SN53cA0cw0m"
      },
      "source": [
        "#### truncatedSVD 결과물로 비교해 보겠습니다.\n",
        "\n",
        "truncatedSVD 속성 `components_` 가 행렬 VT입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZabbHsecw0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ec2616-c581-43ce-f564-2fc403aaa449"
      },
      "source": [
        "components = random_search.best_estimator_.named_steps['svd'].components_\n",
        "print((components - VT).sum())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.2119654035241435e-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOlFnqAzcw0m"
      },
      "source": [
        "TruncatedSVD 속성을 조금 더 살펴봅시다.\n",
        "\n",
        "먼저 특이값를 확인해 보겠습니다. 위에서 구한 S(Sigma)와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB1ajt_Pcw0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6406746-742f-47d8-bbdb-b410833b15c5"
      },
      "source": [
        "components[:10].shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghmd4V1Ocw0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87def5dc-954b-467e-ae17-9532116401f4"
      },
      "source": [
        "print(random_search.best_estimator_.named_steps['svd'].singular_values_)\n",
        "print(S)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.92178091 2.59248335]\n",
            "[3.92178091 2.59248335]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48swUKircw0n"
      },
      "source": [
        "차원이 줄어든 데이터(AV, US)의 분산값입니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDg_b8K4cw0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7805ab2-efa5-4a0d-b0b6-b8a7af06b652"
      },
      "source": [
        "print(random_search.best_estimator_.named_steps['svd'].explained_variance_)\n",
        "print(np.var(US, axis=0))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00302819 0.00629697]\n",
            "[0.00302819 0.00629697]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i-lpQNXcw0n"
      },
      "source": [
        "이번에는 테스트 문서 샘플을 SVD를 사용해 차원 축소해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RFiqWOjcw0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1ed4a2-e13b-472b-bf13-c691d084e89d"
      },
      "source": [
        "# 예시로 테스트 문서 0 벡터를 사용합니다.\n",
        "d0 = X_test[0]\n",
        "print(d0.shape)\n",
        "print(random_search.best_estimator_.named_steps['svd'].transform(d0))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 10000)\n",
            "[[ 0.03407879 -0.03776464]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97d1Ypk3cw0n"
      },
      "source": [
        "테스트 데이터를 모두 변환해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJQI5w9Jcw0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25b0fe6-372e-41b7-8560-0239a0477b41"
      },
      "source": [
        "print(X_test.shape)\n",
        "X_test_trans_2 = random_search.best_estimator_.named_steps['svd'].transform(X_test)\n",
        "print(X_test_trans_2.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(703, 10000)\n",
            "(703, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h938JJbcw0o"
      },
      "source": [
        "위에서 직접 구한 값과 같습니다. (X_test @ VT.T)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8pWdEX6cw0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a0a156-e296-444e-f76c-5d723eccfdd0"
      },
      "source": [
        "# 문서 0만 보겠습니다.\n",
        "X_test_trans[0], X_test_trans_2[0]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(matrix([[ 0.03407879, -0.03776464]]), array([ 0.03407879, -0.03776464]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RakLESJbcw0o"
      },
      "source": [
        "#### LSA는 SVD를 통해 찾아진 topic들을 가지고 문서와, 단어의 잠재적인 의미를 분석하는 것 입니다.\n",
        "\n",
        "각 차원에 어떤 단어들이 모여 있는지 확인해 봅시다.\n",
        "SVD를 통해 찾아진 두 잠재적 의미군(토픽)에 속하는 단어들을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBzTOqeqcw0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2455cd5f-398b-4ca4-d528-de153d55f46f"
      },
      "source": [
        "# terms: 벡터화한 단어\n",
        "terms = random_search.best_estimator_.named_steps['vect'].get_feature_names()\n",
        "for index, topic in enumerate(components[:10]): # topic 최대 10개만 표시, 지금은 k가 2이므로 토픽 2개 다 프린트)\n",
        "    print('Topic %d: '%(index + 1), [terms[i] for i in topic.argsort()[::-1][:6]]) # 토픽에 가장 큰게 기여한 단어부터 최대 6단어 표시\n",
        "    print('Score %d: '%(index + 1), [topic[i] for i in topic.argsort()[::-1][:6]]) # 단어 스코어"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 1:  ['people', 'don', 'like', 'know', 'just', 'think']\n",
            "Score 1:  [0.1812089691771802, 0.1480755846368723, 0.14591065367394312, 0.1395040623730916, 0.13370844711918825, 0.1236837187427575]\n",
            "Topic 2:  ['thanks', 'output', 'ingr', 'dtmedin', 'catbyte', 'circuit']\n",
            "Score 2:  [0.12865724561119146, 0.11374062140626552, 0.10726694807306968, 0.10726694807306968, 0.10726694807306968, 0.10172948540074012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqd6ojDPcw0q"
      },
      "source": [
        "#### MLP(Multi-layer perceptron classifier)를 간단히 사용해보겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BFROfLCcw0q"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf = MLPClassifier(solver='lbfgs'\n",
        "                   , alpha=1e-5\n",
        "                   , hidden_layer_sizes=(16,2)\n",
        "                   , random_state=2\n",
        "                   )"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj8OKDBQ21i8"
      },
      "source": [
        "# 1.Tfidf 문서 벡터화, 2. svd 차원축소, 3. 랜덤포레스트 분류기\n",
        "pipe = Pipeline([\n",
        "    ('vect', vect)\n",
        "    , ('svd', svd)\n",
        "    , ('clf', clf)\n",
        "])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dXvk2_V21jJ",
        "outputId": "935a4b66-3d29-4aca-a0d0-66528e4d1994"
      },
      "source": [
        "# Fit\n",
        "random_search = RandomizedSearchCV(pipe,params, cv=3, n_iter=5, n_jobs=-1, verbose=1)\n",
        "random_search.fit(ng_train.data, ng_train.target)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    7.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=Pipeline(memory=None,\n",
              "                                      steps=[('vect',\n",
              "                                              TfidfVectorizer(analyzer='word',\n",
              "                                                              binary=False,\n",
              "                                                              decode_error='strict',\n",
              "                                                              dtype=<class 'numpy.float64'>,\n",
              "                                                              encoding='utf-8',\n",
              "                                                              input='content',\n",
              "                                                              lowercase=True,\n",
              "                                                              max_df=0.7,\n",
              "                                                              max_features=10000,\n",
              "                                                              min_df=2,\n",
              "                                                              ngram_range=(1,\n",
              "                                                                           2),\n",
              "                                                              norm='l2',\n",
              "                                                              preprocessor=None,\n",
              "                                                              smooth_idf=True,\n",
              "                                                              stop_words='english...\n",
              "                                                            shuffle=True,\n",
              "                                                            solver='lbfgs',\n",
              "                                                            tol=0.0001,\n",
              "                                                            validation_fraction=0.1,\n",
              "                                                            verbose=False,\n",
              "                                                            warm_start=False))],\n",
              "                                      verbose=False),\n",
              "                   iid='deprecated', n_iter=5, n_jobs=-1,\n",
              "                   param_distributions={'svd__n_components': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f436735f250>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2YYRszt3LQ3",
        "outputId": "a5135b8e-6d41-4e54-fe4e-593555051a84"
      },
      "source": [
        "# 테스트셋으로 정확도를 계산합니다\n",
        "y_test = random_search.predict(ng_test.data)\n",
        "accuracy_score(ng_test.target, y_test)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9246088193456614"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bakOb5go3LRE",
        "outputId": "370db352-5596-42c7-8b4f-4268e857cb2c"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(ng_test.target, y_test))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.93       393\n",
            "           1       0.94      0.89      0.91       310\n",
            "\n",
            "    accuracy                           0.92       703\n",
            "   macro avg       0.93      0.92      0.92       703\n",
            "weighted avg       0.93      0.92      0.92       703\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jShS032cw0r",
        "outputId": "fce7ab2a-6107-4472-b5ed-71797c81bf85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf.get_params()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'alpha': 1e-05,\n",
              " 'batch_size': 'auto',\n",
              " 'beta_1': 0.9,\n",
              " 'beta_2': 0.999,\n",
              " 'early_stopping': False,\n",
              " 'epsilon': 1e-08,\n",
              " 'hidden_layer_sizes': (16, 2),\n",
              " 'learning_rate': 'constant',\n",
              " 'learning_rate_init': 0.001,\n",
              " 'max_fun': 15000,\n",
              " 'max_iter': 200,\n",
              " 'momentum': 0.9,\n",
              " 'n_iter_no_change': 10,\n",
              " 'nesterovs_momentum': True,\n",
              " 'power_t': 0.5,\n",
              " 'random_state': 2,\n",
              " 'shuffle': True,\n",
              " 'solver': 'lbfgs',\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': False,\n",
              " 'warm_start': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja8N-1lZc-jw"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvuOc9NOhA1t"
      },
      "source": [
        "## 참고자료\n",
        "\n",
        "- [NLTK Book](https://www.nltk.org/book/)\n",
        "- [KD tree algorithm: how it works](https://youtu.be/TLxWtXEbtFE)\n",
        "- [k-NN 7: how to make it faster](https://youtu.be/sCvJ3zo4DUA)\n",
        "- [An Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)\n",
        "- [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "- [A step by step explanation of PCA(principal component analysis)](http://localhost:8888/?token=ebeea486e072a985fe9597e449bd0f9cecbaec6ae8648532)\n",
        "\n",
        "### Text Classification\n",
        "- [Text Classification](https://developers.google.com/machine-learning/guides/text-classification)\n",
        "- [Text Classification using scikit-learn](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)\n",
        "\n",
        "### SVD\n",
        "- [특이값 분해(SVD)](https://angeloyeo.github.io/2019/08/01/SVD.html)\n",
        "- [Singular Value Decomposition (the SVD)](https://youtu.be/mBcLRGuAFUk)\n",
        "- [특이값 분해(Singular Value Decomposition, SVD)의 활용](https://darkpgmr.tistory.com/106)\n",
        "- [Singular Value Decomposition (SVD) tutorial](https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)\n",
        "- [numpy 벡터와 행렬연산 참고자료](https://ebbnflow.tistory.com/159)\n",
        "- [Image Compression with SVD](http://fourier.eng.hmc.edu/e161/lectures/svdcompression.html)\n",
        "- [Latent semantic indexing](https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html)"
      ]
    }
  ]
}