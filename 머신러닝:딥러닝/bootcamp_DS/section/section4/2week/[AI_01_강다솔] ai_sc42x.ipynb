{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy_9hARVUo9L"
   },
   "source": [
    "# SC42x \n",
    "## 자연어처리 (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCdwXQCtRX4I"
   },
   "source": [
    "# Part 1 : 개념 요약\n",
    "\n",
    "> 다음의 키워드(총 9개)에 대해서 **한 줄**로 간단하게 요약해주세요.\n",
    "\n",
    "**N421**\n",
    "- Stop words(불용어) : I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는 데는 거의 기여하는 바가 없는 단어들을 의미한다.  \n",
    "- 통계적 트리밍 : 기존에 알려진 불용어를 제거하는 대신 코퍼스에서 통계적인 방법을 통해 단어를 제거하는 방법\n",
    "- Stemming과 Lemmatization\n",
    "  * stemming은 어간추출로써 어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해 내는 것을 의미한다.  \n",
    "  * Lemmatization는 단어들이 다른 형태를 가지고 있더라도 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지를 판단하는 것입니다.(즉, 형태학적 파싱)\n",
    "\n",
    "**N422**\n",
    "- CountVectorizer : 단어들의 출현 빈도로 여러개의 문서를 벡터화하는 함수\n",
    "- TfidfVectorizer :여러 개의 문서가 있을 때, 각각의 문서의 내에 있는 단어들에 수치값을 주는 방법인데, 가중치가 적용되어있다. TF-IDF를 계산하면 문서 내에 상대적으로 중요한 단어를 알 수 있다.\n",
    "- LSA : 데이터의 차원을 축소해 문서들에 숨어있는(latent) 의미(Topics)를 끌어내는 방법\n",
    "\n",
    "**N423**\n",
    "- word2vec : 백터화된 문장의 문맥 정보를 보존하기 백터들의 분산을 이용하여 유사도 예측하는 분석방법입니다. 그중 CBOW와 skip-gram이라는 2가지 방법을 학습하였으며, CBOW는 문맥단어들을 가지고 타겟 단어 하나를 맞추는 방법이며, skip-gram은 cbow와 반대되는 개념입니다. (skip-gram : 중간에 있는 단어로 주변 단어를 예측함)\n",
    "\n",
    "- RNN : RNN은 기존의 신경망에서는 입력 순서를 고려한 학습 모델입니다. RNN이 파생된 이유는 시계열 데이터를 적절하게 표현할 수 없기 때문입니다.\n",
    "\n",
    "- LSTM, GRU\n",
    " * LSTM :  RNN에 Gate를 추가한 모델\n",
    " * GRU : 게이트 메커니즘이 적용된 rnn 프레임워크 일종으로 LSTM보다 더 간략한 구조를 가지고 있는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW-JMX7TRX4J"
   },
   "source": [
    "# Part 2 : Yelp 데이터 다루기\n",
    "\n",
    "한 주간 자연어처리 기법을 배우면서 여러분은 다양한 기술들을 접했습니다. 어떻게 텍스트 데이터를 다뤄야 하는지, 텍스트를 벡터화 하는 법, 문서에서 토픽을 모델하는 법 등 다양한 NLP 기법을 배웠는데요. 이번 스프린트 챌린지에선 가장 유명한 NLP 데이터셋 중 하나인 [Yelp](https://www.yelp.com/dataset)의 리뷰 데이터셋을 통해 배운 것들을 복습해보는 시간을 갖겠습니다. \n",
    "\n",
    "실제 데이터셋은 매우 방대하기 때문에 이번 스프린트 챌린지에선 샘플링을 통해 일부의 데이터만 다루도록 하겠습니다. 관심이 있는 분들은 스프린트 챌린지 이후에 다뤄보시기 바랍니다. (미리 분석하고 싶거나 알아보고 싶은 부분을 노트로 남겨두면 이후에 도움이 될 것 같습니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "mCV8yQzRUo9b",
    "outputId": "76ea4393-2f44-44f2-abde-13013e6f1077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n",
       "      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n",
       "      <td>-MhfebM0QIsKt87iDN-FNw</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>2015-04-15 05:21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n",
       "      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n",
       "      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>2013-12-07 03:16:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n",
       "      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n",
       "      <td>HQl28KMwrEKHqhFrrDqVNQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>2015-12-05 03:18:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n",
       "      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n",
       "      <td>5JxlZaqCnk1MnbgRirs40Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>2011-05-27 05:30:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "      <td>IS4cv902ykd8wj1TR0N3-A</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  xQY8N_XvtGbearJ5X4QryQ  OwjRMXRC0KyPrIlcjaXeFQ  -MhfebM0QIsKt87iDN-FNw   \n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA  nIJD_7ZXHq-FX8byPMOkMQ  lbrU8StCq3yDfr-QMnGrmQ   \n",
       "2  LG2ZaYiOgpr2DK_90pYjNw  V34qejxNsCbcgD8C0HVk-Q  HQl28KMwrEKHqhFrrDqVNQ   \n",
       "3  i6g_oA9Yf9Y31qt0wibXpw  ofKDkJKXSKZXu5xJNGiiBQ  5JxlZaqCnk1MnbgRirs40Q   \n",
       "4  6TdNDKywdbjoTkizeMce8A  UgMW8bLE0QMJDCkQ1Ax5Mg  IS4cv902ykd8wj1TR0N3-A   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      2       5      0     0   \n",
       "1      1       1      1     0   \n",
       "2      5       1      0     0   \n",
       "3      1       0      0     0   \n",
       "4      4       0      0     0   \n",
       "\n",
       "                                                text                date  \n",
       "0  As someone who has worked with many museums, I... 2015-04-15 05:21:16  \n",
       "1  I am actually horrified this place is still in... 2013-12-07 03:16:52  \n",
       "2  I love Deagan's. I do. I really do. The atmosp... 2015-12-05 03:18:11  \n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g... 2011-05-27 05:30:52  \n",
       "4  Oh happy day, finally have a Canes near my cas... 2017-01-14 21:56:57  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# yelp 데이터셋을 불러옵니다.\n",
    "yelp = pd.read_json('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/yelp/yelp_reviews_sc41x.json')\n",
    "print(yelp.shape)\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbxEHBanUo9d"
   },
   "source": [
    "## 2.1 `tokenize` 함수를 완성하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "d8tnw2xsRw3O",
    "outputId": "45fe9357-8a5f-4204-96ee-bd5a7b8a77bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n",
       "      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n",
       "      <td>-MhfebM0QIsKt87iDN-FNw</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>2015-04-15 05:21:16</td>\n",
       "      <td>as someone who has worked with many museums i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n",
       "      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n",
       "      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>2013-12-07 03:16:52</td>\n",
       "      <td>i am actually horrified this place is still in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n",
       "      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n",
       "      <td>HQl28KMwrEKHqhFrrDqVNQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>2015-12-05 03:18:11</td>\n",
       "      <td>i love deagans i do i really do the atmosphere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n",
       "      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n",
       "      <td>5JxlZaqCnk1MnbgRirs40Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>2011-05-27 05:30:52</td>\n",
       "      <td>dismal lukewarm defrostedtasting texmex glopmu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "      <td>IS4cv902ykd8wj1TR0N3-A</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "      <td>oh happy day finally have a canes near my casa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  xQY8N_XvtGbearJ5X4QryQ  OwjRMXRC0KyPrIlcjaXeFQ  -MhfebM0QIsKt87iDN-FNw   \n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA  nIJD_7ZXHq-FX8byPMOkMQ  lbrU8StCq3yDfr-QMnGrmQ   \n",
       "2  LG2ZaYiOgpr2DK_90pYjNw  V34qejxNsCbcgD8C0HVk-Q  HQl28KMwrEKHqhFrrDqVNQ   \n",
       "3  i6g_oA9Yf9Y31qt0wibXpw  ofKDkJKXSKZXu5xJNGiiBQ  5JxlZaqCnk1MnbgRirs40Q   \n",
       "4  6TdNDKywdbjoTkizeMce8A  UgMW8bLE0QMJDCkQ1Ax5Mg  IS4cv902ykd8wj1TR0N3-A   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      2       5      0     0   \n",
       "1      1       1      1     0   \n",
       "2      5       1      0     0   \n",
       "3      1       0      0     0   \n",
       "4      4       0      0     0   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  As someone who has worked with many museums, I... 2015-04-15 05:21:16   \n",
       "1  I am actually horrified this place is still in... 2013-12-07 03:16:52   \n",
       "2  I love Deagan's. I do. I really do. The atmosp... 2015-12-05 03:18:11   \n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g... 2011-05-27 05:30:52   \n",
       "4  Oh happy day, finally have a Canes near my cas... 2017-01-14 21:56:57   \n",
       "\n",
       "                                   text_preprocessed  \n",
       "0  as someone who has worked with many museums i ...  \n",
       "1  i am actually horrified this place is still in...  \n",
       "2  i love deagans i do i really do the atmosphere...  \n",
       "3  dismal lukewarm defrostedtasting texmex glopmu...  \n",
       "4  oh happy day finally have a canes near my casa...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄바꿈제거코드 \n",
    "yelp['text_preprocessed'] = yelp['text'].apply(lambda x: x.replace('\\n',''))\n",
    "\n",
    "# 정규화을 사용한 대문자->소문자치환 및  특수문자 제거\n",
    "yelp['text_preprocessed'] = yelp['text'].apply(lambda x:re.sub(r\"[^a-zA-Z0-9 ]\", \"\", x)).str.lower()\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-Vq_-WnyRX4N"
   },
   "outputs": [],
   "source": [
    "# en_core_web_sm load\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vh04YaEZRX4N"
   },
   "outputs": [],
   "source": [
    "# spacy tokenizer 함수 zhem \n",
    "def tokenize(document):\n",
    "    doc = nlp(document)\n",
    "    # punctuations: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.is_alpha == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyTE6xqEUo9e"
   },
   "source": [
    "## 2.2 : 리뷰들을 벡터로 표현하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "d5WMyo7sRX4N",
    "outputId": "493b32e0-b3fb-4b75-d9a2-29479bc13251"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['nt'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>ac</th>\n",
       "      <th>acai</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>yr</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  ability  able  absolute  absolutely  ac  acai  accent  accept  \\\n",
       "0    0        0     0         0           0   0     0       0       0   \n",
       "1    0        0     0         0           0   0     0       0       0   \n",
       "2    0        0     0         0           0   0     0       0       0   \n",
       "3    0        0     0         0           0   0     0       0       0   \n",
       "4    0        0     0         0           0   0     0       0       0   \n",
       "\n",
       "   acceptable  ...  york  young  yr  yuck  yum  yummy  zero  zombie  zone  \\\n",
       "0           0  ...     0      0   0     0    0      0     0       0     0   \n",
       "1           0  ...     0      0   0     0    0      0     0       0     0   \n",
       "2           0  ...     0      0   0     0    0      0     0       0     0   \n",
       "3           0  ...     0      0   0     0    0      0     0       0     0   \n",
       "4           0  ...     0      0   0     0    0      0     0       0     0   \n",
       "\n",
       "   zucchini  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 4000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer를 이용한 백터화 \n",
    "vect = CountVectorizer(stop_words='english'\n",
    "                        ,tokenizer=tokenize\n",
    "                        ,max_features = 4000\n",
    "                       )\n",
    "\n",
    "dtm = vect.fit_transform(yelp['text_preprocessed'])\n",
    "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBRkoidRUo9e"
   },
   "source": [
    "## 2.3 가짜 리뷰를 작성한 후에 비슷한 리뷰를 10개 출력해보세요. 어떤 패턴을 발견하셨나요?\n",
    "> 데이터셋의 사이즈로 인해 `NearestNeighbors` 모델을 사용하는 것을 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_JnIvLrLUo9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 문장 :  it was good\n",
      "------------------------------------\n",
      "1번째로 문장 : \n",
      " -good location\n",
      "-good prices\n",
      "-clean rooms\n",
      "-affordable food\n",
      "-friendly and helpful staff\n",
      "-NO BEDBUGS!!!\n",
      "\n",
      "\n",
      "2번째로 문장 : \n",
      " :)\n",
      "\n",
      "\n",
      "3번째로 문장 : \n",
      " 去吃了烧烤了，鱼豆腐好好吃，外酥里嫩！肉也很好吃，一进去服务员都很热情！有种宾至如归的感觉！环境也很好！我还吃了绝代双骄，就是我想要的那个味道！特别推荐大家去试试，外婆家红烧肉，也就是我记忆中的味道！很正宗！以后会常来的！希望可以继续保持哦！\n",
      "\n",
      "\n",
      "4번째로 문장 : \n",
      " 如果做创新餐厅要丢掉传统菜的口味和优点，那还是不要创新了。天妇罗互相之间粘在一起，鳗鱼卷两个两个粘着而且米饭是松松的。不想说了。扫兴\n",
      "\n",
      "\n",
      "5번째로 문장 : \n",
      " Good food.\n",
      "\n",
      "\n",
      "6번째로 문장 : \n",
      " タイ料理屋さん。\n",
      "夕食分と朝食分を持ち帰りで注文。自分、日本人なんだけど、辛さはどのくらいが良さそう？って聞いて、ミディアムにしたんだけど、ミディアムでもだいぶ辛かった。。。もいちょい下のランクにしても良いかも。\n",
      "でも、麺もカレーもどちらも美味しかったので、また行きたいお店。\n",
      "\n",
      "\n",
      "7번째로 문장 : \n",
      " A good experience\n",
      "\n",
      "\n",
      "8번째로 문장 : \n",
      " 入店時に精算用のカードを手渡され、好きな席に座り、キッチンやバーに好きなものを買いに行きます。\n",
      "ワインが豊富。\n",
      "パスタは麺の種類がえらべ、ピザはナポリっぽい感じで美味しいです。\n",
      "サラダの味もなかなかでした。\n",
      "カジュアルで、でも美味しく気軽に入れるお店です。\n",
      "帰りにカードを精算します。\n",
      "なのでチップも不要。\n",
      "お一人でもどうぞ。\n",
      "\n",
      "\n",
      "9번째로 문장 : \n",
      " 这个地方很棒，我喜欢他们的玉米棒！\n",
      "\n",
      "\n",
      "10번째로 문장 : \n",
      " 周末请饮茶，听讲唔错，琳住吃好d就选择离呢度。12点几仍然要等位，假期无办法啦。门口入去大厅会见到隐蔽鱼缸养住鲜活大皇帝蟹和大龙虾。餐馆大厅几大气，高大上，特别是寿司吧的位置。可能晚左离，想试招牌点心黑金流沙包卖晒了，点了黑金奶油糕，其实就系黑芝麻糕加奶油，因为都几大一件，好饱肚，少少痴口。翡翠杂菌饺和樱花虾墨鱼卖都ok.可口萝卜海鲜馃原来系炸既，少少热气。价钱稍微偏贵\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
    "nn.fit(dtm)\n",
    "\n",
    "# Test\n",
    "example_st = \"it was good\"\n",
    "\n",
    "test_st = vect.transform([example_st])\n",
    "\n",
    "# print full text \n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "\n",
    "# result\n",
    "print(\"Test 문장 : \" ,example_st)\n",
    "print(\"------------------------------------\")\n",
    "for index_nm, row in enumerate(nn.kneighbors(test_st.todense())[1][0]):\n",
    "    print(\"{0}번째 문장 : \\n {1}\".format(index_nm+1, yelp['text'][row]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5VG_MwRUo9f"
   },
   "source": [
    "## 2.4 리뷰의 별 갯수 ('stars')를 예측하는 분류 Pipeline을 구축하세요.\n",
    "> `CountVectorizer` 혹은 `TfidfVector`가 포함 되있어야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VmFJWL9OUo9g"
   },
   "outputs": [],
   "source": [
    "# 기본 라이브러리 \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# 분석 라이브러리 \n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 구축 및 평가 라이브러리 \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MyJ3-kYlbsXw"
   },
   "outputs": [],
   "source": [
    "# rf_model 구현\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vect = CountVectorizer(stop_words='english',\n",
    "                      ngram_range=(1,2),\n",
    "                      min_df=2,\n",
    "                      max_df=0.6,\n",
    "                      max_features=5000)\n",
    "\n",
    "svd_model = TruncatedSVD(algorithm='randomized',\n",
    "                  n_iter=5,\n",
    "                  random_state=2)\n",
    "\n",
    "rf_model =RandomForestClassifier(n_estimators=500, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IxN7u52qbt23"
   },
   "outputs": [],
   "source": [
    "params = {'svd__n_components' : stats.randint(2, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cURr7U7ZbvIg"
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', vect)\n",
    "    , ('svd', svd_model)\n",
    "    , ('clf', rf_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNd9b1y2b9O3",
    "outputId": "121b114a-c86b-48ee-cc1f-8149ee0bc3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000,)\n"
     ]
    }
   ],
   "source": [
    "target = yelp['stars']\n",
    "train = yelp['text_preprocessed']\n",
    "print(train.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOu6Aqzrbxrn",
    "outputId": "d04ead7f-b8fb-4ed6-95e9-c2d53a91e8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (10000,)\n",
      "(40000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=.2, random_state=2)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hU_x5mttb_Ir",
    "outputId": "6544dfd5-b4e3-4feb-836d-058baef4a543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('vect',\n",
       "                                              CountVectorizer(max_df=0.6,\n",
       "                                                              max_features=5000,\n",
       "                                                              min_df=2,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2),\n",
       "                                                              stop_words='english')),\n",
       "                                             ('svd',\n",
       "                                              TruncatedSVD(random_state=2)),\n",
       "                                             ('clf',\n",
       "                                              RandomForestClassifier(n_estimators=500,\n",
       "                                                                     random_state=2))]),\n",
       "                   n_iter=5, n_jobs=-1,\n",
       "                   param_distributions={'svd__n_components': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001DED297A3A0>},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_model = RandomizedSearchCV(pipe, params, cv=3, n_iter=5, n_jobs=-1, verbose=1)\n",
    "random_search_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTnAQ0ddcCNw",
    "outputId": "99ec73de-2266-4e62-b798-98b34540d120"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3937"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = random_search_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpX4fYnUo9g"
   },
   "source": [
    "## 2.5 구축한 Pipeline으로 2.3에서 작성한 가짜 리뷰의 별 갯수를 예측하세요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8OUt-LPUo9g",
    "outputId": "d8942421-19f3-487f-a045-bc7949fea0ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "print(random_search_model.predict([example_st]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuK5mUAgUo9g"
   },
   "source": [
    "## 2.6 `GridSearchCV` 혹은 `RandomizedSearchCV`를 활용하여 Pipeline을 튜닝해서 성능을 개선하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6m_fkQJKhvGI"
   },
   "source": [
    "위에서 한번에 진행함 \n",
    "아래 램던서치문제가 있는지 사전에 읽지 못하여 한번에 구현을 진행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QesyQ19Uo9h"
   },
   "source": [
    "# Part 3 : Deep Learning for Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmnTVJoNU-VE"
   },
   "source": [
    ">LSTMs로 무엇을 할 수 있을까요? **시퀀스**를 분석하고 있기 때문에, 우리는 분류 이상의 것을 할 수 있습니다. <br>\n",
    ">Kaggle에서 Text Summarization 문제를 해결한 노트북을 참고하여 우리가 배운 내용들이 실제로 어떻게 사용되는지 확인해보고 **모델 생성 (아키텍쳐 파트)과 관련된 내용**에 한글로 주석을 달아봅시다.\n",
    "\n",
    "[Data Link](https://www.kaggle.com/sunnysai12345/news-summary) <br/>\n",
    "[Notebook Example Link](https://www.kaggle.com/sandeepbhogaraju/text-summarization-with-seq2seq-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qne89enkeVIe"
   },
   "source": [
    "1. 전처리를 진행한 상태 \n",
    "2. 인코더 구현 \n",
    "3. 성능향상을 위해 lstm을 3개 쌓아서 모델을 구성함 \n",
    "4. 임베딩 layer 생성 \n",
    "5. 디코더 구현\n",
    "6. 각 스텝마다 cost (오류)를 계산해서 하위 스텝으로 오류를 전파하여 각 weight를 업데이트하기 위해 TimeDistributed로 dense생성\n",
    "7. rmsprop으로 최적화를 하고 다중분류이기 때문에 sparse_categorical_crossentropy로 loss함수 정의함\n",
    "8. 튀는 현상 및 학습속도향상을 위해 EarlyStopping을 사용함\n",
    "9. 모델 예측을 위한 초기화 \n",
    "10. 목표 어휘 이상의 prob dist를 생성을 위해 고밀도 소프트 맥스 레이어 정의\n",
    "11. 요약 및 리뷰를 위해 정수 시퀀스를 단어 시퀀스로 변환하는 함수정의\n",
    "12. 모델실행 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRn44qjhUo9j"
   },
   "source": [
    "# Advanced Goals: 3점을 획득하기 위해선 아래의 조건 중 하나 이상을 만족해야합니다\n",
    " \n",
    "- Part 2에서 vectorization 방법들을 모두 사용한 후 비교해보세요\n",
    "- SC에서 사용한 데이터보다 더 큰 Yelp 데이터셋을 분석해보세요 (데이터가 매우 크기 때문에 파일을 읽어올 때 주의하시길 바랍니다: $\\approx$ 6.6+ GB)\n",
    "- Part 3에서 사용한 노트북 전체에 주석을 달아봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lz2uaXFYUo9j",
    "outputId": "b052ad9a-70e4-4c2a-d486-0605d2dd03fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('tfid',\n",
       "                                              TfidfVectorizer(max_df=0.6,\n",
       "                                                              max_features=5000,\n",
       "                                                              min_df=2,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2),\n",
       "                                                              stop_words='english')),\n",
       "                                             ('svd',\n",
       "                                              TruncatedSVD(random_state=2)),\n",
       "                                             ('clf',\n",
       "                                              RandomForestClassifier(n_estimators=500,\n",
       "                                                                     random_state=2))]),\n",
       "                   n_iter=5, n_jobs=-1,\n",
       "                   param_distributions={'svd__n_components': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001DEE0712310>},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(stop_words='english',\n",
    "                      ngram_range=(1,2),\n",
    "                      min_df=2,\n",
    "                      max_df=0.6,\n",
    "                      max_features=5000)\n",
    "\n",
    "svd_model = TruncatedSVD(algorithm='randomized',\n",
    "                  n_iter=5,\n",
    "                  random_state=2)\n",
    "\n",
    "rf_model =RandomForestClassifier(n_estimators=500, random_state=2)\n",
    "\n",
    "params = {'svd__n_components' : stats.randint(2, 3)}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfid', tfid)\n",
    "    , ('svd', svd_model)\n",
    "    , ('clf', rf_model)\n",
    "])\n",
    "\n",
    "random_search_model = RandomizedSearchCV(pipe, params, cv=3, n_iter=5, n_jobs=-1, verbose=1)\n",
    "random_search_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9K-TAVUse7qp",
    "outputId": "eff41ff5-2e0b-4650-ff50-740c5bad7087"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4063"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = random_search_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IiXKYq1ae_TQ",
    "outputId": "329e6150-4917-43d1-b817-28364b2a207b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_model.predict([example_st]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTWRmJsGf-by"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ai-sc42x.ipynb",
   "provenance": []
  },
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.22.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
