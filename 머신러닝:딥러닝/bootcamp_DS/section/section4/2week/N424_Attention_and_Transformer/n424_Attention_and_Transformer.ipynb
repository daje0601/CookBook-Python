{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n424_Attention_and_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "d9ixL-TF_uWx",
        "y_Sn-XoHANda",
        "j6lc3U9wBSrg",
        "XZo2_s-Z2Vi2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ixL-TF_uWx"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## ***DATA SCIENCE / SECTION 4 / SPRINT 2 / NOTE 4***\n",
        "\n",
        "---\n",
        "\n",
        "# Attention, Transformer & Others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Sn-XoHANda"
      },
      "source": [
        "## Warm Up\n",
        "\n",
        "지난 시간에 배웠던 내용을 떠올려봅시다.\n",
        "\n",
        "- RNN, LSTM, GRU\n",
        "\n",
        "    - RNN 기반 모델의 장점에 대해서 생각해봅시다.\n",
        "    - RNN 기반 모델의 단점에 대해서 생각해봅시다. (2가지 이상)\n",
        "        - LSTM과 GRU는 어떤 단점을 어떻게 극복하였는지 다시 알아봅시다.\n",
        "\n",
        "- 이번 시간에는 Attention과 Transformer에 대해서 배울 예정입니다.\n",
        "    - [Attention이 결합된 Seq2Seq 모델](https://youtu.be/WsQLdu2JMgI) 소개 영상\n",
        "        - Attention : 주의, 집중\n",
        "        - 번역할 때 어떤 단어에 주의, 집중 해야할 지를 알려주는 것이 Attention\n",
        "    - [Transformer](https://www.youtube.com/watch?v=mxGCEWOxfe8) 소개 영상\n",
        "        - RNN과는 완전히 다른 모델\n",
        "        - ***Attention is All You Need : 필요한 건 Attention 뿐***\n",
        "    - [GPT](https://www.youtube.com/watch?v=FeEmmylAF0o) 소개 영상\n",
        "        - 사전 학습 언어 모델\n",
        "        - GPT의 구조\n",
        "    - [BERT](https://youtu.be/vo3cyr_8eDQ?t=712) 소개 영상 (11:52 부터)\n",
        "        - BERT의 구조\n",
        "        - MLM(Masked Self-Attention)\n",
        "        - NSP(Next Sentence Prediction)\n",
        "\n",
        "- 자연어 처리의 다양한 태스크에 대해서 알아봅시다. 다음 키워드 중 관심있는 것을 구글링하여 알아봅시다. \n",
        "    - 자연어 이해(Natural Language Understanding, NLU)\n",
        "        - 감성 분석 및 문서 분류\n",
        "        - 자연어 추론(Natural Language Inference, NLI)\n",
        "        - 기계 독해(Machine Reading Comprehension, MRC)\n",
        "    - 자연어 생성(Natural Language Generation, NLG)\n",
        "    - 기계 번역(Machine Translation)\n",
        "    - TTS(Text to Speech), STT(Speech to Text)\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6lc3U9wBSrg"
      },
      "source": [
        "## 🏆 학습 목표\n",
        "\n",
        "- **Attention 메커니즘을 이해하고 어떤 문제를 해결했는지 알 수 있다.**\n",
        "    - Attention 메커니즘이 무엇이며 기계번역(Machine Translation) 성능을 끌어올렸는지 알 수 있다.\n",
        "    - Attention 으로도 해결할 수 없는 RNN 기반 모델의 단점에 대해서 알 수 있다.\n",
        "\n",
        "- **Transformer의 장점과 주요 프로세스인 Self-Attention에 대해 이해하고 설명할 수 있다.** \n",
        "    - 트랜스포머를 발표한 논문 제목은 왜 \"Attention is All You Need\"인지 설명할 수 있다.\n",
        "    - Positional Encoding을 적용하는 이유에 대해서 설명할 수 있다.\n",
        "    - Masked Self-Attention가 트랜스포머 구조 중 어디에 적용되며 어떤 역할을 하는지 설명할 수 있다. \n",
        "    - 기존 RNN과 비교하여 Transformer가 가지는 장점에 대해서 설명할 수 있다.\n",
        "\n",
        "- **GPT, BERT 그리고 다른 모델에 대해서 개략적으로 설명할 수 있다.**\n",
        "    - GPT(Generative Pre-Training)\n",
        "        - 사전 학습된 언어 모델(Pre-trained LM)의 Pre-training과 Fine-tuning은 무엇이고 각각 어떤 종류의 데이터셋을 사용하는 지 설명할 수 있다.\n",
        "        - GPT는 Transformer를 어떻게 변형하였는지 설명할 수 있다.\n",
        "    - BERT(Bidirectional Encoder Representation by Transformer)\n",
        "        - BERT는 Transformer를 어떻게 변형하였으며 GPT와의 차이 무엇인지 알 수 있다.\n",
        "        - MLM(Masked Language Model)은 무엇인지 이해할 수 있다.\n",
        "        - NSP(Next Sentence Prediction)은 무엇인지 이해할 수 있다.\n",
        "    - 최근 언어 모델의 발전은 어떻게 진행되고 있는지 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01tsYPjM_yH-"
      },
      "source": [
        "## RNN with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COGueQEOGZdv"
      },
      "source": [
        "RNN이 가진 가장 큰 단점 중 하나는 **장기 의존성(Long-term dependency)** 문제입니다.<br/>장기 의존성 문제란 문장이 길어질 경우 앞 단어의 정보를 잃어버리게 되는 현상입니다.<br/>장기 의존성 문제를 해결하기 위해 나온 것이 셀 구조를 개선한 LSTM과 GRU입니다.<br/>기계 번역에서 RNN 기반의 모델(LSTM, GRU)이 단어를 처리하는 방법은 아래와 같다고 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8mTUUb7BJV2"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86040995-f27b4800-ba7f-11ea-8ca1-67b2517573eb.gif\" alt=\"seq2seq_6\" width=\"800\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cubjTVyGxuuh"
      },
      "source": [
        "하지만 LSTM, GRU로 개선하였더라도 문장이 길어지면 모든 단어 정보를 고정 길이의 Hidden state 벡터에 담기 어렵다는 단점을 가지고 있습니다.<br/>\n",
        "많은 단어의 의미를 벡터 하나에 담기엔 부족하기 때문입니다.<br/>\n",
        "이런 문제를 해결하기 위해서 고안된 방법이 바로 **Attention** 입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx5ZqJaYgoWe"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86040873-b942d800-ba7f-11ea-9f59-ee23923f777e.gif\" alt=\"seq2seq_7\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1SEDqkfYmU8"
      },
      "source": [
        "Attention은 각 인코더의 Time-step 마다 생성되는 Hidden state 벡터를 간직합니다.<br/>\n",
        "입력 단어가 N개라면 그만큼의 Hidden state 벡터를 모두 간직하게 됩니다.<br/>\n",
        "모든 단어가 입력되면 생성된 Hidden state 벡터를 모두 디코더에 넘겨줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Furq4hDZPG"
      },
      "source": [
        "디코더에서는 단어를 생성할 때마다 인코더에서 넘어온 모든 Hidden state 벡터와 얼마나 관련이 있는지 가중치를 구합니다.<br/>이 때 디코더의 Hidden state 벡터와 인코더의 각 Hidden state 벡터를 내적하여 가중치를 구하게 됩니다.<br/>아래는 디코더 첫 단어 \"I\"(`Time-step 4`)에 대한 어텐션 가중치가 구해지는 과정입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmGvkTW7M1eE"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86044868-ae8b4180-ba85-11ea-9fee-2977edfd47ce.gif\" alt=\"seq2seq_img\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Iz1neANg5L"
      },
      "source": [
        "디코더는 인코더에서 넘어온 모든 Hidden state 벡터에 대해 위와 같은 계산을 실시합니다.<br/>\n",
        "그렇기 때문에 Time-step마다 출력할 단어(Target word)가 어떤 Hidden state 벡터와 연관이 있는 지, 즉 어떤 단어에 **집중(Attention)**할 지를 알 수 있습니다.<br/>이런 과정을 거쳐 단어를 생성하면 인코더에 입력되는 모든 단어의 정보를 활용할 수 있습니다.<br/>즉, 장기 의존성 문제를 해결할 수 있습니다.<br/>예시로 제시되었던 문장(`Je suis etudiant => I am a student`)을 번역했을 때 각 단어마다의 Attention 스코어를 시각화하면 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ5o7VwCOpQg"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86047018-29a22700-ba89-11ea-98ee-a90b2fb70a23.gif\" alt=\"attn_visualization\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXn15KKCffXx"
      },
      "source": [
        "Attention을 구현한 코드를 실행해보도록 하겠습니다.\n",
        "\n",
        "- **모듈 임포트 / 텍스트 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-gJBkcrfcM7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Sm3kyRfuyu",
        "outputId": "1d7665e1-ded3-45d3-f229-af5acff765fc"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roj-oDN9fz3D"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                 if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ljYaCtQf4VT",
        "outputId": "a159c514-fb32-46cb-ff6c-c928e62b25ef"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra76W5Hkf7QP"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
        "                for line in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZLq0gWxf9_R",
        "outputId": "70d6fe22-cbdb-48e4-e982-09ff062e316d"
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwo7oj6WgAY5"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqPktihKgCqs"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xJxnsOviQ7Y"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJzfJMD2iRm7",
        "outputId": "a4897624-9bf2-4978-d4f8-7cbd000f8e00"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNcJ5CiHgo7M"
      },
      "source": [
        "- **설정하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gyhFLuRg0VP"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ngvIQ5xg2WP",
        "outputId": "510c044f-99e0-47ac-fcd5-7ea2c66f305a"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoFkIUrQhEjZ"
      },
      "source": [
        "- **인코더 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrVGkNVYhDkG"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0p3wzwMhNbb",
        "outputId": "05824a4b-8e3c-4275-8a92-d41356ca8336"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
        "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoB8eAG_hPls"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yej3jZPdhSCL",
        "outputId": "7cb9b8ca-5815-41cf-9fed-a71daa0e2413"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gui7f8kVhYhC"
      },
      "source": [
        "- **디코더 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V_tdtO2hT1b"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lkg44lphcGZ",
        "outputId": "3ad33517-e0e5-4f00-f1d7-2c6e4fa2c8e8"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGEjBTgDheMw"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUhuss6ChgiY"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MRQ6Ulhh0VW",
        "outputId": "a04a676a-548a-4963-d3ee-582f70e462b5"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "      \n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.7960\n",
            "Epoch 1 Batch 100 Loss 2.2292\n",
            "Epoch 1 Batch 200 Loss 1.9146\n",
            "Epoch 1 Batch 300 Loss 1.6447\n",
            "Epoch 1 Loss 2.0216\n",
            "Time taken for 1 epoch 45.51616978645325 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4850\n",
            "Epoch 2 Batch 100 Loss 1.4446\n",
            "Epoch 2 Batch 200 Loss 1.3863\n",
            "Epoch 2 Batch 300 Loss 1.2924\n",
            "Epoch 2 Loss 1.3546\n",
            "Time taken for 1 epoch 34.42869186401367 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.0562\n",
            "Epoch 3 Batch 100 Loss 0.9927\n",
            "Epoch 3 Batch 200 Loss 0.8976\n",
            "Epoch 3 Batch 300 Loss 0.8325\n",
            "Epoch 3 Loss 0.9220\n",
            "Time taken for 1 epoch 34.383094787597656 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7657\n",
            "Epoch 4 Batch 100 Loss 0.7361\n",
            "Epoch 4 Batch 200 Loss 0.6516\n",
            "Epoch 4 Batch 300 Loss 0.5745\n",
            "Epoch 4 Loss 0.6181\n",
            "Time taken for 1 epoch 34.198145627975464 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4225\n",
            "Epoch 5 Batch 100 Loss 0.5018\n",
            "Epoch 5 Batch 200 Loss 0.4357\n",
            "Epoch 5 Batch 300 Loss 0.4594\n",
            "Epoch 5 Loss 0.4176\n",
            "Time taken for 1 epoch 34.26620602607727 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.3250\n",
            "Epoch 6 Batch 100 Loss 0.2546\n",
            "Epoch 6 Batch 200 Loss 0.3414\n",
            "Epoch 6 Batch 300 Loss 0.3458\n",
            "Epoch 6 Loss 0.2867\n",
            "Time taken for 1 epoch 34.169394969940186 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2158\n",
            "Epoch 7 Batch 100 Loss 0.1921\n",
            "Epoch 7 Batch 200 Loss 0.1899\n",
            "Epoch 7 Batch 300 Loss 0.1386\n",
            "Epoch 7 Loss 0.2041\n",
            "Time taken for 1 epoch 34.23347210884094 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1367\n",
            "Epoch 8 Batch 100 Loss 0.1689\n",
            "Epoch 8 Batch 200 Loss 0.1746\n",
            "Epoch 8 Batch 300 Loss 0.2270\n",
            "Epoch 8 Loss 0.1526\n",
            "Time taken for 1 epoch 34.12479901313782 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1603\n",
            "Epoch 9 Batch 100 Loss 0.1013\n",
            "Epoch 9 Batch 200 Loss 0.1304\n",
            "Epoch 9 Batch 300 Loss 0.1260\n",
            "Epoch 9 Loss 0.1181\n",
            "Time taken for 1 epoch 34.020458698272705 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0900\n",
            "Epoch 10 Batch 100 Loss 0.0797\n",
            "Epoch 10 Batch 200 Loss 0.0944\n",
            "Epoch 10 Batch 300 Loss 0.1101\n",
            "Epoch 10 Loss 0.0954\n",
            "Time taken for 1 epoch 33.88239026069641 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7aJioMoiYx1"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L9MXJY9iZhv"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLJ37OOOib_L"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "93IpAAFMi9Yb",
        "outputId": "695a756c-59b8-4151-a1b1-88007b66e39e"
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: it s very cold here . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "_uQW40H-jAYu",
        "outputId": "53a5ec39-05de-458f-af7d-c75f9c31d6b5"
      },
      "source": [
        "translate(u'esta es mi vida.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my life . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y9W20irk9mO"
      },
      "source": [
        "-----------------------------\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpSQ4utmO71g"
      },
      "source": [
        "## Transformer : Attention is All You Need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_Nkvve_FFIG"
      },
      "source": [
        "<img src=\"https://fastly.syfy.com/sites/syfy/files/styles/2280x1280_hero/public/2020/01/transformers-last-knight.jpg?offset-x=0&offset-y=0\" alt=\"transformer_electric\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKH7yo-AO3Ph"
      },
      "source": [
        "트랜스포머는 기계 번역을 위한 새로운 모델로 이전에 등장했던 Attention 메커니즘을 극대화하여 뛰어난 번역 성능을 기록했습니다.<br/>최근 자연어처리 모델 SOTA(State-of-the-Art)의 기본 아이디어는 모두 트랜스포머를 기반으로 하고 있습니다.<br/>모델을 소개한 논문 [Attention is All You Need](https://arxiv.org/abs/1706.03762) 는 3년 사이에 18000번 이상 인용되었습니다.<br/> 트랜스포머가 자연어처리가 아닌 다른 문제도 잘 풀고있기 때문에 최근에는 컴퓨터 비전 쪽에서도 적용하려는 시도가 있으며, 멀티모달(Multi-Modal) 모델에도 적용되고 있습니다.<br/>\n",
        "\n",
        "Attention을 적용하였든 그렇지 않든, RNN 기반 모델이 가진 특징은 단어가 **순서대로** 들어온다는 점입니다.<br/>처리해야 하는 시퀀스가 길수록 **연산 속도가 느려**집니다. **트랜스포머는 이런 문제를 해결하기 위해 등장한 모델**입니다.<br/>모든 토큰을 동시에 입력받아 병렬 연산하기 때문에 단어가 입력되기를 기다리지 않아도 된다는 장점이 있습니다.\n",
        "\n",
        "아래는 트랜스포머의 구조를 단순하게 시각화한 그림입니다.<br/>인코더 블록과 디코더 블록이 6개씩 모여있습니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEjnrUq-Zq9X"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" alt=\"positional_encoding\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNUm_DCUO_Yn"
      },
      "source": [
        "그림 하나를 더 보도록 하겠습니다. 아래는 논문이 제시한 트랜스포머의 구조입니다.<br/>그림을 보면 커다란 회색 블록이 2개 있습니다.<br/>왼쪽은 인코더 블록 하나를 나타내고 오른쪽은 디코더 블록 하나를 나타냅니다.<br/>인코더 블록은 크게 2개의 sub-layer **[`Multihead (Self) Attention`, `Feed Forward`]** 로 나눌 수 있습니다.<br/>디코더 블록은 3개의 sub-layer **[`Masked Multihead (Self) Attention`, `Multihead (Encoder-Decoder) Attention` `Feed Forward`]** 로 나눌 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf5KKTsuPENw"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" alt=\"positional_encoding\" width=\"550\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntqpucgWuOKh"
      },
      "source": [
        "### Positional Encoding (위치 인코딩)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO2N2G4pPiA3"
      },
      "source": [
        "<img width=\"400\" alt=\"pe\" src=\"https://user-images.githubusercontent.com/45377884/112799904-ecb3a100-90a9-11eb-9072-87a965e81a77.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw_uLipBPnMG"
      },
      "source": [
        "트랜스포머에서는 모든 단어가 동시에 입력됩니다.<br/>그래서 단어의 위치 정보를 제공하기 위한 벡터를 따로 제공해주어야 합니다.<br/>단어의 상대적인 위치 정보를 제공하기 위한 벡터를 만드는 과정을 `Positional Encoding` 이라고 합니다.<br/>`Positional Encoding`은 아래와 같은 수식으로 이루어집니다. 수식이 복잡하니 일단은 식을 이해하려고 하지 않으셔도 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQLVS_0zPpdA"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{PE}_{\\text{pos},2i} &= \\sin \\bigg(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\bigg) \\\\\n",
        "\\text{PE}_{\\text{pos},2i+1} &= \\cos \\bigg(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\bigg)\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqVM-W4y3cCG"
      },
      "source": [
        "아래 `Positional Encoding`을 시각화한 자료로부터 일정한 패턴이 있는 벡터임을 볼 수 있습니다.<br/>이를 통해 단어의 상대적인 위치를 파악 `Positional Encoding`이 있는 이유를 이해하는 것에 집중하도록 합시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZXLdjSfPva1"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png\" alt=\"positional_encoding\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yD70CNguVAC"
      },
      "source": [
        "### Self-Attention\n",
        "***(N434에서 이것만이라도 제대로 알고 넘어갑시다)***\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gEVfOCmPycD"
      },
      "source": [
        "<img width=\"300\" alt=\"self-Attn\" src=\"https://user-images.githubusercontent.com/45377884/112809266-ca735080-90b4-11eb-9a25-7f34f37880c7.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2AxXXfPP0wH"
      },
      "source": [
        "**Self-Attention** 은 트랜스포머의 주요 메커니즘입니다.\n",
        "\n",
        "> *The animal didn't cross the street because <ins>it</ins> was too tired* \n",
        "\n",
        "위와 같은 문장을 제대로 번역하려면 **_\"it\"_** 과 같은 지시대명사가 어떤 대상을 가리키는지 알아야 합니다.<br/>번역하려는 문장 내부 요소의 관계를 잘 파악하기 위해서 문장 자신에 대해 어텐션 메커니즘을 적용합니다.<br/>이를 `Self-Attention`이라고 합니다.<br/>아래는 **_\"it\"_** 이 어떤 단어와 가장 연관되어 있는 지를 시각화한 그림입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvAsFAWJP2xz"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\" alt=\"self_attention_visualization\" width=\"350\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG-Z6J9qwpyL"
      },
      "source": [
        "\n",
        "**`Self-Attention`**은 어떤 과정이길래 단어 사이의 관계를 알아낼 수 있을까요?<br/>비밀은 **3가지 가중치 벡터**에 있습니다.<br/>각 벡터는 **쿼리(Query), 키(Key), 밸류(Value)**라고 부릅니다.<br/>RNN의 Hidden state 벡터를 대체하기 위한 가중치 벡터라고 생각하시면 됩니다.<br/> 각각의 벡터가 어떤 역할을 하는지 알아보겠습니다.\n",
        "\n",
        "- **쿼리(q)**는 분석하고자 하는 단어에 대한 가중치 벡터입니다.\n",
        "\n",
        "- **키(k)**는 각 단어가 해당 쿼리와 얼마나 연관있는 지를 비교하기 위한 가중치 벡터입니다. \n",
        "\n",
        "- **밸류(v)**는 각 단어의 의미를 살려주기 위한 가중치 벡터입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrXC04GQy4Yb"
      },
      "source": [
        "**`Self-Attention`**은 세 가지 가중치 벡터를 대상으로 어텐션을 적용합니다.\n",
        "\n",
        "1. 먼저, **쿼리(q)와 모든 단어의 키(k) 벡터를 내적**합니다. 내적을 통해 나오는 값이 Attention 스코어(Score)가 됩니다.\n",
        "\n",
        "2. 트랜스포머에서는 이 가중치를 q,k,v 벡터 차원 $d_k$ 의 제곱근 $\\sqrt{d_k}$ 로 나누어줍니다. 계산값을 안정적으로 만들어주기 위한 계산 보정으로 생각해주시면 됩니다.  \n",
        "\n",
        "3. 다음으로 **Softmax**를 취해주면 쿼리에 해당하는 단어와 문장 내 다른 단어가 가지는 관계의 비율을 구할 수 있습니다.\n",
        "\n",
        "4. 마지막으로 **밸류(v) 각 단어의 벡터를 곱해준 후 모두 더하면** Self-Attention 과정이 마무리됩니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XUQi85yy7kR"
      },
      "source": [
        "**`Self-Attention`** 의 과정을 그림으로 다시 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTREy_tA1tAc"
      },
      "source": [
        "**1. 가중치 행렬 $W^Q, W^K, W^V$ 로부터 각 단어의 쿼리, 키, 밸류(q, k, v) 벡터를 만들어냅니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBOvyUgjRE6C"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-1.png\" alt=\"transformer_15\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH4SjvhPQB0h"
      },
      "source": [
        "**2. 분석하고자 하는 단어의 쿼리 벡터(q)와 문장 내 모든 단어(자신 포함)의 키 벡터(k)를 내적하여 각 단어와 얼마나 관련 정도를 구합니다.**\n",
        "\n",
        "(아래 그림에서는 $\\sqrt{d_k}$로 나누어 준 뒤에 Softmax를 취해주는 과정은 생략되었습니다.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKrYd1eWRIH7"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-2.png\" alt=\"transformer_15\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ_HnY9vQD4o"
      },
      "source": [
        "**3.  Softmax의 출력값과밸류 벡터(v)를 곱해준 뒤 더하면 해당 단어에 대한 Self-Attention 출력값을 얻을 수 있습니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKbpOuTVRKf0"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-3.png\" alt=\"transformer_15\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iMflDTbQFV4"
      },
      "source": [
        "**4. 하나의 벡터에 대해서만 살펴보았지만 실제 Attention 계산은 행렬 단위로 병렬 계산됩니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMO4BdoQRM7P"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-summary.png\" alt=\"transformer_15\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj6LqffDxYZu"
      },
      "source": [
        "실제로 각 벡터는 **행렬(Q, K, V)**로 한꺼번에 계산됩니다. $W^Q, W^K, W^V$ 는 학습 과정에서 갱신되는 파라미터로 이루어진 행렬입니다. 세 행렬과 단어 행렬을 내적하여 쿼리, 키, 밸류 행렬(Q, K, V)를 만들어냅니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un25Mx6_RPem"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation.png\" alt=\"transformer_12\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0_CnhEVQI4h"
      },
      "source": [
        "위에서 살펴본 바와 같이\n",
        "\n",
        "1. 먼저 쿼리 행렬(Q)과 키 행렬(K)을 **내적**합니다.\n",
        "\n",
        "2. 결과로 나오는 행렬의 요소를 $\\sqrt{d_k}$ 로 **나누어 줍니다.**\n",
        "\n",
        "3. 행렬의 각 요소에 `Softmax`를 취해줍니다. \n",
        "\n",
        "4. 마지막으로 **밸류 행렬(V)과 내적**하면 최종 결과 행렬(Z)이 반환됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uinyklXWyLrn"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" alt=\"transformer_13\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpxgcGIgj5PC"
      },
      "source": [
        "아래는 `Tensorflow` 에서 Self-Attention을 구현한 코드입니다. 코드를 통해 Attention이 계산되는 과정을 다시 살펴보도록 합시다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFkqQMjHpIxI"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_RmjSVYluaO"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTRaQ5oVQ1gh"
      },
      "source": [
        "다음으로 **`Multi-Head Attention`** 에 대해 알아보겠습니다.<br/>**`Multi-Head Attention`** 이란 **`Self-Attention`** 을 동시에 여러 개로 실행하는 것입니다.<br/>각 Head 마다 다른 Attention 결과를 내어주기 때문에 앙상블과 유사한 효과를 얻을 수 있습니다.<br/> 논문에서는 8개의 Head를 사용하였습니다.<br/>8번의 Self-Attention을 실행하여 각각의 출력 행렬 $Z_0, Z_1, \\cdots , Z_7$ 을 만들어냅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rvzeSQkQ4SJ"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_z.png\" alt=\"transformer_16\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTcLefocQ7mC"
      },
      "source": [
        "출력된 행렬 $Z_n (n=0,\\cdots,7)$ 은 **이어붙여집니다(Concatenate)**.<br/>또 다른 파라미터 행렬인 $W^o$ 와의 내적을 통해 Multi-Head Attention의 최종 결과인 행렬 $Z$를 만들어냅니다.<br/>여기서 행렬 $W^o$의 요소 역시 학습을 통해 갱신됩니다.<br/>최종적으로 생성된 행렬 $Z$는 토큰 벡터로 이루어진 행렬 $X$와 **동일한 크기(Shape)**가 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkS1oMy6Q-Pc"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\" alt=\"transformer_17\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZo2_s-Z2Vi2"
      },
      "source": [
        "### Layer Normalization & Skip Connection\n",
        "\n",
        "<img width=\"300\" alt=\"lnorm_resicon\" src=\"https://user-images.githubusercontent.com/45377884/113169444-9056aa00-9280-11eb-8ba0-17c9211ad412.png\">\n",
        "\n",
        "트랜스포머의 모든 sub-layer에서 출력된 벡터는 **Layer normalization**과 **Skip connection**을 거치게 됩니다.<br/>Layer normalization의 효과는 Batch normalization과 유사합니다. 학습이 훨씬 빠르고 잘 되도록 합니다.<br/>Skip connection(혹은 Residual connection)은 역전파 과정에서 정보가 소실되지 않도록 합니다.<br/>Sprint3에서 배울 ResNet의 주요 메커니즘이므로 해당 부분에서 더욱 자세하게 다룰 예정입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQPefZjI2UTM"
      },
      "source": [
        "### Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHwk8VeqmNzU"
      },
      "source": [
        "<img width=\"300\" alt=\"스크린샷 2021-03-29 오후 5 27 32\" src=\"https://user-images.githubusercontent.com/45377884/112808809-58027080-90b4-11eb-8ca7-ffa38e577d3d.png\">\n",
        "\n",
        "다음으로 **`FFNN(Feed forward neural network)`** 로 들어갑니다.<br/>은닉층의 차원이 늘어났다가 다시 원래 차원으로 줄어드는 단순한 2층 신경망입니다.<br/>활성화 함수(Activation function)으로 ReLU를 사용합니다.\n",
        "\n",
        "$$\n",
        "\\text{FFNN}(x) = \\max(0, W_1x + b_1) W_2 +b_2\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzFup_5pDupE"
      },
      "source": [
        "### Masked Self-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_kn1-AYR18G"
      },
      "source": [
        "<img width=\"300\" alt=\"Masked_Self-Attention_in_structure\" src=\"https://user-images.githubusercontent.com/45377884/112808936-78322f80-90b4-11eb-9315-22cd9caad41d.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNdrsLrER3w2"
      },
      "source": [
        "**Masked Self-Attention**은 디코더 블록에서 사용되는 특수한 Self-Attention입니다.<br/> 디코더는 Auto Regressive 하게 단어를 생성하기 때문에 타깃 단어 이후 단어를 보지 않고 단어를 예측해야 합니다.<br/>따라서 타깃 단어 뒤에 위치한 단어는 Self-Attention에 영향을 주지 않도록 **마스킹(masking)**을 해주어야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8-UwtUcR5WO"
      },
      "source": [
        "<img width=\"500\" alt=\"Masked_Self-Attention_ex\" src=\"http://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33J9FxItR70W"
      },
      "source": [
        "***Self-Attention (without Masking) vs Masked Self-Attention***\n",
        "\n",
        "<img width=\"500\" alt=\"Masked_Self-Attention_ex2\" src=\"http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJfqdxHSAY3"
      },
      "source": [
        "원래 Self-Attention 메커니즘은 쿼리 행렬(Q)와 키 행렬(K)의 내적으로 나온 행렬을 차원의 제곱근 $\\sqrt{d_k}$ 로 나누어 준 다음 Softmax를 취해주고 밸류 행렬(V)과 내적하였습니다.\n",
        "\n",
        "**`Masked Self-Attention`** 에서는 Softmax를 취해주기 전, 가려주고자 하는 요소에만 $-\\infty$ 에 해당하는 매우 작은 수를 더해줍니다.<br/>아래 예시에서는 -10억(=-1e9)을 더해주었습니다.<br/>이 과정을 **마스킹(Masking)**이라고 합니다.<br/>마스킹된 값은 Softmax를 취해 주었을 때 0이 나오므로 Attention 메커니즘에 반영되지 않습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbTQf8oxzth-"
      },
      "source": [
        "<img width=\"600\" alt=\"masked_1\" src=\"http://jalammar.github.io/images/gpt2/transformer-attention-mask.png\">\n",
        "\n",
        "<img width=\"600\" alt=\"masked_2\" src=\"http://jalammar.github.io/images/gpt2/transformer-attention-masked-scores-softmax.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98r-D1WjSH3w"
      },
      "source": [
        "위에서 등장했던 Self-Attention을 구현 코드에서. `mask` 와 관련된 부분만 다시 보도록 합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03EldudrJgAD"
      },
      "source": [
        "# def scaled_dot_product_attention(q, k, v, mask):\n",
        "#   \"\"\"Calculate the attention weights.\n",
        "#   q, k, v must have matching leading dimensions.\n",
        "#   k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "#   The mask has different shapes depending on its type(padding or look ahead) \n",
        "#   but it must be broadcastable for addition.\n",
        "  \n",
        "#   Args:\n",
        "#     q: query shape == (..., seq_len_q, depth)\n",
        "#     k: key shape == (..., seq_len_k, depth)\n",
        "#     v: value shape == (..., seq_len_v, depth_v)\n",
        "#     mask: Float tensor with shape broadcastable \n",
        "#           to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "#   Returns:\n",
        "#     output, attention_weights\n",
        "#   \"\"\"\n",
        "\n",
        "#   matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "#   # scale matmul_qk\n",
        "#   dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "#   scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    \n",
        "  \"\"\"\n",
        "    mask가 있을 경우 masking된 자리에는 (-inf)에 해당하는 절댓값이 큰 음수 -1e9(=-10억)을 더해줍니다.\n",
        "    그 값에 softmax를 취해주면 거의 0에 가까운 값이 나옵니다. 그 다음 value 계산시에 반영되지 않습니다.\n",
        "  \"\"\"\n",
        "    \n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "\n",
        "#   # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "#   # add up to 1.\n",
        "#   attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "#   output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "#   return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OSVcyskH543"
      },
      "source": [
        "### Encoder-Decoder Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUGAg5ikSb_F"
      },
      "source": [
        "<img width=\"300\" alt=\"Encoder-Decoder_Attention\" src=\"https://user-images.githubusercontent.com/45377884/112809435-f8f12b80-90b4-11eb-96e1-3b0f7c530659.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wepMQd4oSdkE"
      },
      "source": [
        "디코더에서 Masked Self-Attention 층을 지난 벡터는 **Encoder-Decoder Attention** 층으로 들어갑니다.<br/>좋은 번역을 위해서는 번역할 문장과 번역된 문장 간의 관계 역시 중요합니다.<br/> 번역할 문장과 번역되는 문장의 정보 관계를 엮어주는 부분이 바로 이 부분입니다.\n",
        "\n",
        "이 층에서는 **디코더 블록의** Masked Self-Attention으로부터 출력된 벡터를 **쿼리(Q)** 벡터로 사용합니다.<br/>**키(K)와 밸류(V)** 벡터는 최상위(=6번째) 인코더 블록에서 사용했던 값을 그대로 가져와서 사용합니다.<br/>**`Encoder-Decoder Attention`** 층의 계산 과정은 Self-Attention 했던 것과 동일합니다.\n",
        "\n",
        "아래는 **`Encoder-Decoder Attention`** 가 진행되는 순서를 나타낸 이미지입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kawL4lmnSh0v"
      },
      "source": [
        "<img width=\"700\" alt=\"Encoder-Decoder_Attention_gif\" src=\"http://jalammar.github.io/images/t/transformer_decoding_1.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOn_vpeyRyyT"
      },
      "source": [
        "### Linear & Softmax Layer\n",
        "\n",
        "이제 끝입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYcth4VySk4T"
      },
      "source": [
        "<img width=\"300\" alt=\"Linear_Softmax\" src=\"https://user-images.githubusercontent.com/45377884/112815762-994a4e80-90bb-11eb-9a57-a8be65c1a30b.png\">\n",
        "\n",
        "디코더의 최상층을 통과한 벡터들은 Linear 층을 지난 후 Softmax를 통해 예측할 단어의 확률을 구하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBx1WzlmRwcM"
      },
      "source": [
        "### 코드 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_-WgBYX5-4K"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX26HWbe6Cph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "4237c92d-3aa5-4585-e448-e9fe1209b9ec"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-146109541dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTransformerBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.ffn = keras.Sequential(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_VY_WXJ6FdH"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvOXtcK76JV8"
      },
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py_M438h6OPe"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y38uKp96TQf"
      },
      "source": [
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ9M6E4rk4xc"
      },
      "source": [
        "--------------------\n",
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb-St9gHFVQX"
      },
      "source": [
        "## GPT, BERT & Others\n",
        "\n",
        "이번에 배울 GPT와 BERT는 트랜스포머 구조의 일부분을 변형하여 만들어진 언어 모델입니다. 두 모델은 **사전 학습된 언어 모델(Pre-trained Language Model)** 이라는 공통점을 가지고 있습니다. 사전 학습이란 대량의 데이터를 사용하여 미리 학습하는 과정인데요. 여기에 필요한 데이터를 추가 학습시켜 모델의 성능을 최적화합니다. 이런 학습 방법을 전이 학습(Transfer Learning)이라고도 합니다. 최근 발표되고 있는 SOTA(최고 성능) 언어 모델은 모두 전이 학습을 사용한 모델입니다. \n",
        "\n",
        "이번 시간에는 대표적인 두 가지 사전 학습된 언어 모델, GPT와 BERT에 대해 개략적으로 알아보겠습니다. 그리고 이런 모델을 기반으로 최근 발표되는 모델의 경향성에 대해서도 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz7HMOrtGwJD"
      },
      "source": [
        "### GPT (2018.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aiUjhboTxmy"
      },
      "source": [
        "<img width=\"300\" alt=\"Linear_Softmax\" src=\"https://openai.com/content/images/2019/05/openai-cover.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZdn1m3mTy87"
      },
      "source": [
        "**GPT**는 **G**enerative **P**re-trained **T**ransformer 의 약자로 2018년 6월에 OpenAI를 통해 발표되었습니다.<br/>연이어 발표한 GPT-2(2019.2), GPT-3(2020.6)가 좋은 성능을 보이면서 세간의 주목을 받았습니다.<br/>기사를 첨부해드리니 영상이 끝나고 추가적으로 읽어보시면 좋겠습니다. \n",
        "\n",
        "- 심화내용 : Generative란? (당장은 이해하지 못하셔도 좋습니다) - [discriminative vs generative](https://ratsgo.github.io/generative%20model/2017/12/17/compare/)\n",
        "\n",
        "- GPT2 기사\n",
        "    - [The AI that was too dangerous to release](https://blog.floydhub.com/gpt2/)\n",
        "    - [OpenAI, 공유하기에는 너무 위험한 ‘텍스트 생성 AI’의 진실](http://www.aitimes.com/news/articleView.html?idxno=121589)\n",
        "\n",
        "- GPT3 기사\n",
        "    - [A GPT-3 bot posted comments on Reddit for a week and no one noticed](https://www.technologyreview.com/2020/10/08/1009845/a-gpt-3-bot-posted-comments-on-reddit-for-a-week-and-no-one-noticed/)\n",
        "    - [GPT3가 쓴 뉴스가 랭킹 1위, 사람을 이겼다](http://www.aitimes.com/news/articleView.html?idxno=131593)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqhHhR8r8NLF"
      },
      "source": [
        "GPT-1, GPT-2, GPT-3가 전부 동일하지는 않지만 기본적으로 동일한 구조를 가지고 있습니다.\n",
        "\n",
        "GPT의 구조를 알아보기 전에 기본이 되는 아이디어인 **사전 학습(Pre-training)** 이라는 아이디어에 대해서 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIvqSVD4wMnW"
      },
      "source": [
        "- **사전 학습된 언어 모델 (Pre-trained LM)**\n",
        "\n",
        "혹시 아래와 같은 케이스를 본 적 있으신가요?\n",
        "\n",
        "<img width=\"500\" alt=\"pre-train\" src=\"https://user-images.githubusercontent.com/45377884/112774307-3e413900-9074-11eb-94ab-f3bc000ff95e.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkf1OJv6U8Gq"
      },
      "source": [
        "수능 국어 시험에서 책을 많이 읽은 학생은 조금만 공부해도 상위권 성적을 유지하는 경우가 종종 있습니다.<br/> GPT에서 사용된 **사전 학습**이라는 아이디어도 유사한 생각에서 시작되었습니다.<br/>아래 그림을 보며 사전 학습 언어 모델에 대해서 설명을 이어나가겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue0QaxvK63RP"
      },
      "source": [
        "<img width=\"700\" alt=\"Pre-training\" src=\"https://user-images.githubusercontent.com/45377884/112943247-35cc2980-916c-11eb-99be-2fa7657507d2.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSoHE5F8U-d2"
      },
      "source": [
        "사전 학습 언어 모델은 크게 2가지 과정을 통해 완성됩니다. 첫 번째가 **사전 학습(Pre-training)**입니다. \n",
        "\n",
        "존재하는 자연어 중에는 책(Book corpus)이나 위키피디아(Wiki corpus) 등 레이블링 되지 않은 데이터가 더 많습니다.<br/>여러분이 지금 읽고 있는 강의 노트 역시 레이블링 되지 않은 자연어 데이터 입니다.<br/>\n",
        "책을 많이 읽는 것처럼 레이블링 되지 않은 데이터를 모델이 학습하도록 하는 과정을 **사전 학습** 이라고 합니다.\n",
        "\n",
        "사전 학습이 끝난 모델에 우리가 하고자하는 태스크에 특화된(Task specific) 데이터를 학습합니다.<br/> 이를 **Fine-tuning** 이라고 합니다.<br/> Fine-tuning에서는 학습시 레이블링 된 데이터 [Ex) 감성 분석, 자연어 추론(NLI), 질의 응답(QA)] 를 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4zkV_8d6TbJ"
      },
      "source": [
        "- **모델 구조 (그림은 6개의 디코더 블록을 사용하지만 GPT는 12개의 블록을 사용합니다.)**\n",
        "\n",
        "<img width=\"700\" alt=\"Pre-training\" src=\"http://jalammar.github.io/images/xlnet/transformer-decoder-intro.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1N5Jgrt9GNx"
      },
      "source": [
        "GPT에서는 인코더를 사용하지 않기 때문에 디코더 블록내에 **2개의 Sub-layer**만 있습니다. 트랜스포머의 디코더 블록에서는 Masked Self-Attention을 지나서 Encoder-Decoder Attention 층으로 들어갔습니다. 하지만 GPT는 인코더를 사용하지 않으므로 Encoder-Decoder Attention층이 빠지게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqFHif7m8vR8"
      },
      "source": [
        "- **사전 학습(Pre-training)**\n",
        "\n",
        "레이블링 되지 않은 대량의 말뭉치 $U = (u_1, \\cdots , u_n)$ 에 대해 로그 우도 $L_1$ 을 최대화하는 방향으로 학습됩니다. 다음에 올 단어를 계속해서 맞추는 방식으로 학습합니다.\n",
        "\n",
        "$$\n",
        "L_1(U) = \\sum_i \\log P(u_i \\vert u_{i-k}, \\cdots, u_{i-1}; \\Theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOjb7lmd96FT"
      },
      "source": [
        "- **Fine-tuning**\n",
        "\n",
        "기존 모델에서는 태스크에 맞춰 모델 구조를 변경해주고 학습을 진행시켰습니다.\n",
        "\n",
        "하지만 GPT와 같은 사전 학습 언어 모델은 Fine-tuning 과정에서 데이터의 입력 방식만을 변형시키고 모델 구조는 일정하도록 설계되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_0mywdu-ydy"
      },
      "source": [
        "<img width=\"600\" alt=\"fine-tune_structure\" src=\"https://user-images.githubusercontent.com/45377884/112949500-408abc80-9174-11eb-8090-4f0be34db572.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KKJNSrp-0Uk"
      },
      "source": [
        "\n",
        "Fine-tuning은 레이블링 된 말뭉치 $C = (x_1, \\cdots , x_m)$ 에 대하여 로그 우도 $L_2$ 를 최대화하는 방향으로 학습합니다.\n",
        "\n",
        "$$\n",
        "L_2(C) = \\sum_{(x,y)} \\log P(y \\vert x_1, \\cdots , x_m)\n",
        "$$\n",
        "\n",
        "Fine-tuning에서 학습하는 데이터셋이 클 때는 보조 목적함수로 $L_1$ 을 추가하여 $L_3$로 학습하면 학습이 더 잘 진행됩니다.\n",
        "\n",
        "$$\n",
        "L_3(C) = L_2(C) + \\lambda \\cdot L_1(C)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E47CeLkDkb-"
      },
      "source": [
        "- **결과 & 결론**\n",
        "\n",
        "LSTM, GRU를 사용한 기존 모델보다 자연어 추론(NLI), 질의 응답(QA), 분류(Classification) 등의 태스크에서 높은 성능을 달성하였습니다.<br/>GPT는 사전 학습된 언어 모델을 바탕으로 좋은 성능을 확보할 수 있다는 점과 사전 학습 모델에 Transformer 구조가 더 좋은 성능을 보임을 알 수 있었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aHDyu6nq5ir"
      },
      "source": [
        "### BERT (2018.10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6PsrQKgAmyz"
      },
      "source": [
        "<img width=\"600\" alt=\"model_name\" src=\"https://user-images.githubusercontent.com/45377884/112963631-88184500-9182-11eb-8c87-f470e25d7567.png\">\n",
        "\n",
        "> [다양한 캐릭터 이름을 딴 NLP 모델](https://towardsdatascience.com/machine-learnings-obsession-with-kids-tv-show-characters-728edfb43b3c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWjBLUwWAoay"
      },
      "source": [
        "**BERT**(**B**idirectional **E**ncoder **R**epresentation by **T**ransformer)는 2018년 10월 구글에서 발표한 모델입니다.\n",
        "\n",
        "모델 이름에서 알 수 있듯 BERT는 트랜스포머의 인코더만을 사용하여 양방향(Bidirectional)으로 읽어냅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJV0Z4s3dn1a"
      },
      "source": [
        "- **구조**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wo4epFbAqp7"
      },
      "source": [
        "GPT가 트랜스포머의 디코더 블록을 12개 쌓아올린 모델이었다면 **BERT는 트랜스포머의 인코더 블록**을 12개 쌓아올린 모델입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpUIL_WuAspc"
      },
      "source": [
        "<img width=\"500\" alt=\"model_name\" src=\"http://jalammar.github.io/images/bert-base-bert-large-encoders.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwwu_ynaAuC9"
      },
      "source": [
        "BERT 역시 GPT 와 동일한 Pre-trained LM 이기 때문에 Pre-training과 Fine-tuning 과정을 통해 학습됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGnMfe2efIMd"
      },
      "source": [
        "- **사전 학습(Pre-training)**\n",
        "\n",
        "BERT의 특이한 사전 학습 방법을 사용합니다. BERT의 사전 학습 과정에서 사용되는 **2가지 방법(MLM, NSP)**을 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEIRSx5vV_Pt"
      },
      "source": [
        "> **MLM(Masked Language Model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RHHJKmVBrJV"
      },
      "source": [
        "첫 번째는 MLM(Masked Language Model) 입니다. 혹시 아래와 같은 문제를 풀어보신 경험이 있으신가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAkoYiM-Bsmu"
      },
      "source": [
        "<img width=\"300\" alt=\"mlm\" src=\"https://thumb.mt.co.kr/06/2013/11/2013110718224659109_1.jpg/dims/optimize/\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40WZ-DudWEva"
      },
      "source": [
        "영어 시험을 한 번쯤 준비해보신 분이라면 빈칸 채우기 유형 문제를 풀어보신 적이 있을 것입니다. 보통은 빈칸에 문법적/의미적으로 올바른 단어를 채우게 됩니다.\n",
        "\n",
        "BERT도 이처럼 **빈칸 채우기**를 하면서 단어를 학습합니다.<br/>BERT는 사전 학습 과정에서 레이블링 되지 않은 말뭉치 중에서 랜덤으로 15\\%가량의 단어를 마스킹합니다.<br/>그리고 마스킹된 위치에 원래 있던 단어를 예측하는 방식으로 학습을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGQkOv6YWJjB"
      },
      "source": [
        "<img width=\"600\" alt=\"mlm_example\" src=\"http://jalammar.github.io/images/BERT-language-modeling-masked-lm.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFqu5WuaDgJY"
      },
      "source": [
        "MLM은 양쪽의 문맥을 동시에 볼 수 있다는 장점이 있습니다.\n",
        "\n",
        "아래 그림은 GPT와 BERT의 학습 방향을 비교하여 나타낸 그림입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a3g9iZEDkg-"
      },
      "source": [
        "<img width=\"300\" alt=\"gpt_vs_bert\" src=\"https://user-images.githubusercontent.com/45377884/113259927-a445ee80-9308-11eb-8fbd-95d5f553480a.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTT-rlIaDlw7"
      },
      "source": [
        "GPT는 ***'거기'*** 라는 단어를 예측할 때 '어제 카페 갔었어'의 정보만 볼 수 있습니다.<br/>\n",
        "하지만 BERT는 빈칸에 들어갈 ***'거기'*** 라는 단어를 예측 할 때 '어제 카페 갔었어'뿐만 아니라 '사람 많더라'의 정보도 참고할 수 있습니다.<br/>\n",
        "이렇게 양방향으로 학습할 경우 단어가 문맥 사이에서 가진 의미를 최대로 학습할 수 있습니다.\n",
        "\n",
        "MLM은 다소 간단한 아이디어이지만 단어의 문맥적 의미를 최대로 학습할 수 있도록 함으로써 BERT가 높은 성능을 달성하는 데에 커다란 역할을 하였습니다.<br/>다음으로 2번째 방법인 NSP에 대해서 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JmCxVxFWB4d"
      },
      "source": [
        "> **NSP(Next Sentence Prediction)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6Xt904dG11P"
      },
      "source": [
        "BERT는 NSP(Next Sentence Prediction) 방식으로도 학습합니다. 동문서답이라는 사자성어 알고 계신가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVdPbkU2G6uG"
      },
      "source": [
        "<img width=\"350\" alt=\"nsp_idea\" src=\"https://thx.sfo2.cdn.digitaloceanspaces.com/wr/hanja_images/%E6%9D%B1%E5%95%8F%E8%A5%BF%E7%AD%94_800.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEcsClhSXw95"
      },
      "source": [
        "NSP는 모델이 문맥에 맞는 이야기를 하는지 아니면 동문서답을 하는지 아닌지를 판단하며 학습하는 방식입니다.<br/>NSP에 대해 알아보기 전에 BERT에 있는 두 가지 Special Token에 대해 알아보겠습니다.<br/>BERT의 Special Token은 `[SEP]`(Separation)과 `[CLS]`(Classification)이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMJ0csCoIJSn"
      },
      "source": [
        "BERT는 사전 학습 시에 텍스트를 2개로 나눠서 넣게 됩니다.<br/>`[CLS]`는 모든 단어 토큰 앞에 위치하고, `[SEP]`은 두 텍스트 사이와 맨 마지막에 위치합니다.<br/>NSP는 가운데 `[SEP]` 토큰 뒤에 오는 텍스트가 이전 텍스트와 이어지는 부분인지를 `[CLS]`를 통해 예측합니다.\n",
        "\n",
        "두 문장이 바로 이어지는 문장일 경우 **`IsNext`** 로 판단하며 그렇지 않은 문장 쌍일 경우  **`NotNext`** 를 판단하도록 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwnji699X1LR"
      },
      "source": [
        "<img width=\"500\" alt=\"nsp_1\" src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtNsNQHFInjv"
      },
      "source": [
        "아래는 드라마 대본을 예시로 NSP가 어떻게 작동하는 지를 나타낸 그림입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id8g1fpoX-yl"
      },
      "source": [
        "<img width=\"500\" alt=\"nsp_2\" src=\"https://user-images.githubusercontent.com/45377884/86514846-d0067780-be4f-11ea-9809-c3e43b8ad3f9.png\">     \n",
        "\n",
        "<img width=\"500\" alt=\"nsp_3\" src=\"https://user-images.githubusercontent.com/45377884/86514847-d137a480-be4f-11ea-82be-d229bf75fbf8.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrXfjo2MX4RZ"
      },
      "source": [
        "NSP 역시 간단한 아이디어지만 모델이 문장과 문장 사이의 관계를 학습할 수 있도록 함으로써 질의응답(QA), 자연어 추론(NLI) 등의 태스크에서 좋은 성능을 보일 수 있었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8sUh3xCxv_U"
      },
      "source": [
        "- **Fine-tuning**\n",
        "\n",
        "BERT 역시 모델의 구조는 그대로 유지한 채 데이터를 입력하는 형태만 바꾸어서 Fine-tuning을 실시합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6cB2PLbMZc9"
      },
      "source": [
        "<img width=\"700\" alt=\"nsp_2\" src=\"http://jalammar.github.io/images/bert-tasks.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYDed3TaYB_c"
      },
      "source": [
        "(a)는 “Sentence” 쌍을 분류하는 태스크입니다. `[SEP]`으로 나눠진 “Sentence” 쌍을 입력받아 `[CLS]`가 출력하는 클래스를 반환합니다.\n",
        "\n",
        "(b)는 감성분석 등 하나의 문장을 입력하여 `[CLS]`로 해당 문장을 분류하는 태스크입니다.\n",
        "\n",
        "(c)는 질의 응답 태스크입니다. 질문과 본문에 해당하는 단락을 `[SEP]` 토큰으로 나누어 입력하면 질문에 대한 답을 출력하도록 합니다.\n",
        "\n",
        "(d)는 품사 태깅(POS tagging)이나 개체명 인식(Named Entity Recognition, NER) 등의 태스크입니다. 입력받은 각 토큰마다 답을 출력합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc45Gcc80eXj"
      },
      "source": [
        "- **결과 & 결론**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i70diS5cMfR2"
      },
      "source": [
        "BERT는 간단한 사전 학습 아이디어로 많은 태스크에서 SOTA를 달성하였습니다.<br/>단순한 아이디어를 통해 엄청난 성능을 달성하였기에 당시 많은 충격을 주었습니다. 이후로도 BERT를 개선하기 위한 연구가 많이 진행되었습니다.\n",
        "\n",
        "특히 MLM을 통해 BERT가 좋은 성능을 달성한 뒤로 텍스트에 노이즈를 준 후에 이를 다시 맞추는(Denoising) 방법에 대해 많은 연구가 진행되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gweObbaTGpqX"
      },
      "source": [
        "### Beyond BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FfDoAT7Neng"
      },
      "source": [
        "- **더 큰 모델 만들기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQLXjCffdLj9"
      },
      "source": [
        "<img width=\"700\" alt=\"getting_bigger\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caGYeYHZdN3Z"
      },
      "source": [
        "GPT(2018.6)와 BERT(2018.10) 이후로도 수많은 모델이 발표되어 왔습니다. 이후 발표되고 있는 모델의 주요 경향성 중 하나는 **모델 크기 키우기** 입니다.<br/>위 그림에서 볼 수 있듯  GPT와 BERT이후 발표되는 모델의 파라미터 수는 기하급수적으로 증가하고 있습니다.\n",
        "\n",
        "특히 작년 6월에 발표된 GPT-3의 파라미터 개수는 약 1750억 개로 위 그림에 나와있는 T-NLG보다도 10배나 많은 파라미터 수를 가지고 있습니다.<br/>크기를 키울수록 더 좋은 성능을 보여주고 있기 때문에 계속해서 키우고 있는 상황입니다. <br/>하지만 사전 학습에 따른 비용 문제 등 크기만 커지는 모델에 대한 우려의 시각도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ENRgv7ZdQGv"
      },
      "source": [
        "<img width=\"400\" alt=\"getting_bigger_gpt3\" src=\"https://miro.medium.com/max/1164/1*C-KNWQC_wXh-Q2wc6VPK1g.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8iDUH8WsW36"
      },
      "source": [
        "- **더 좋은 학습 방법을 적용하거나 가벼운 모델 만들기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQw_pUUAdXA8"
      },
      "source": [
        "기존 GPT나 BERT의 단점을 보완하는 방향의 연구도 지속되고 있습니다. 트랜스포머의 디코더 블록만을 사용한 GPT와 인코더 블록만을 사용한 BERT는 상대적으로 쓰기(생성)와 읽기(자연어 이해)에 특화된 모델입니다.<br/>**두 모델이 사용했던 방법을 결합하거나 심화**시킨 모델로 더 좋은 성능을 얻을 수 있었습니다.<br/>대표적인 모델로 Masking 방법에 변화를 주는 SpanBERT, RoBERTa나 Noising 방법에 변화를 준 XLNet이나 BART 등의 모델이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIQ7NpPpdZOX"
      },
      "source": [
        "<img width=\"500\" alt=\"bart\" src=\"https://miro.medium.com/max/1400/0*MeyyeTYxwtSZJPiL\">   \n",
        "\n",
        "\n",
        "<img width=\"500\" alt=\"bart_noising\" src=\"https://www.weak-learner.com/assets/img/blog/personal/bart_transformations.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb_aui5ydbXf"
      },
      "source": [
        "모델 자체가 크다 보니 사이즈를 줄이되 성능은 보전하는 쪽으로도 많은 연구가 진행되고 있습니다.<br/> DistillBERT, ALBERT(A Light BERT) 나 Electra가 이런 방향으로 연구된 대표적인 모델이라고 할 수 있습니다.<br/>아래는 GAN-Like 방법을 적용한 Electra의 모델 구조인데요. 이런 모델 모두가 다양한 방법을 이용해서 BERT의 크기를 줄이고 효율성을 높였습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w0WwJPMddHE"
      },
      "source": [
        "\n",
        "<img width=\"500\" alt=\"bart_noising\" src=\"https://1.bp.blogspot.com/-sHybc03nJRo/XmfLongdVYI/AAAAAAAAFbI/a0t5w_zOZ-UtxYaoQlVkmTRsyFJyFddtQCLcBGAsYHQ/s1600/image1.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Op5Ou_RsBB2"
      },
      "source": [
        "- **여러 방면에서의 다양한 시도**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJSbrN17d25l"
      },
      "source": [
        "T5나 GPT-3와 같은 모델은 하나의 모델로 더욱 다양한 태스크를 수행할 수 있는 모델입니다.<br/>특히 GPT-3는 Few-shot learning 방법론을 적용한 모델로 적당한 길이의 제시문만 주어주면 Fine-tuning 없이도 엄청나게 좋은 성능을 보여줍니다.<br/>N-shot learning 에 대해서는 아래 자료를 참고하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQp0fL9EeWqL"
      },
      "source": [
        "1. ***파인튜닝(finetuning)*** : 다운스트림 태스크에 해당하는 데이터 전체를 사용합니다. 모델 전체를 다운스트림 데이터에 맞게 업데이트합니다.\n",
        "\n",
        "2. ***제로샷러닝(zero-shot learning)*** : 다운스트림 태스크 데이터를 전혀 사용하지 않습니다. 모델이 바로 다운스트림 태스크를 수행합니다.\n",
        "\n",
        "3. ***원샷러닝(one-shot learning)*** : 다운스트림 태스크 데이터를 한 건만 사용합니다. 모델 전체를 1건의 데이터에 맞게 업데이트합니다. 업테이트 없이 수행하는 원샷러닝도 있습니다. 모델이 1건의 데이터가 어떻게 수행되는지 참고한 뒤 바로 다운스트림 태스크를 수행합니다.\n",
        "\n",
        "4. ***퓨샷러닝(few-shot learning)*** : 다운스트림 태스크 데이터를 몇 건만 사용합니다. 모델 전체를 몇 건의 데이터에 맞게 업데이트합니다. 업데이트 없이 수행하는 퓨삿러닝도 있습니다. 모델이 몇 건의 데이터가 어떻게 수행되는지 참고한 뒤 바로 다운스트림 태스크를 수행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16QSbKw43OKP"
      },
      "source": [
        "**다국어(multilingual) 모델** 역시 열심히 연구되고 있는 분야입니다.<br/>보통의 경우 단일 말뭉치로만 사전 학습을 진행하기 때문에 여러 언어를 사용하고자 하면 성능이 급격히 저하되는 경우가 많습니다.\n",
        "\n",
        "다양한 태스크에서 언어를 넘나들며 사용할 수 있는 모델 역시 많이 연구되고 있습니다.<br/>대표적인 모델로는 mBART(multi-lingual BART), mT5(multi-lingual T5) 등이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wia-8pdid6k9"
      },
      "source": [
        "\n",
        "\n",
        "지난 1월에는 GPT를 발표했던 OpenAI에서 DALL-E 라는 재미있는 모델을 발표했습니다. 이 모델은 텍스트로부터 이미지를 생성합니다.<br/>이렇게 자연어를 넘어 다양한 매체로 기계와 소통하는 **멀티모달(Multi-Modal)**에 대한 연구도 활발하게 진행되고 있습니다.<br/>특히 트랜스포머가 멀티모달 문제를 푸는 데에 굉장히 좋은 성능을 보이고 있기 때문에 이를 활용하여 다양한 문제를 풀고자 하고있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObY9pMs2d8Sh"
      },
      "source": [
        "<img width=\"700\" alt=\"dall-e\" src=\"https://user-images.githubusercontent.com/45377884/113083201-b9425500-9216-11eb-989a-3e5f28a794e5.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPgdtKE03lGX"
      },
      "source": [
        "## Review\n",
        "\n",
        "학습 목표에 대해 다시 생각해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t22B1RW63zTq"
      },
      "source": [
        "- Attention의 장점에 대해서 생각하고 설명해봅니다.\n",
        "\n",
        "    - RNN 모델의 단점 2가지\n",
        "    - 장기 의존성(Long-term dependency)\n",
        "    - Attention의 장점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMlh3bET4NRZ"
      },
      "source": [
        "- Transformer의 장점과 구조에 대해서 생각하고 설명해봅니다.\n",
        "    - \"Attention is All You Need\" (왜 논문 제목을 이렇게 지었을지에 대해서 생각해봅시다)\n",
        "    - Positional Encoding\n",
        "    - Self-Attention\n",
        "    - Masked Self-Attention\n",
        "    - Encoder-Decoder Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XOZcD7x6SR2"
      },
      "source": [
        "- GPT & BERT\n",
        "    - 사전 학습 언어 모델(Pretrained Language Model), 전이 학습(Transfer Learning)\n",
        "        - 사전 학습(Pre-training)\n",
        "        - Fine-tuning\n",
        "    - GPT의 구조\n",
        "    - BERT의 구조\n",
        "        - MLM(Masked Langauge Model)\n",
        "        - NSP(Next Sentence Prediction)\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQlhRzWQsQN1"
      },
      "source": [
        "## 참고 자료"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHL_Cr3IA0IL"
      },
      "source": [
        "- Attention에 대해 자세하게 알고 싶다면\n",
        "    - [Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) (Mechanics of Seq2seq Models With Attention)\n",
        "    - [번역](https://nlpinkorean.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
        "    - [Paper](https://arxiv.org/pdf/1409.0473.pdf) (Neural machine translation by jointly learning to align and translate)\n",
        "\n",
        "- 트랜스포머에 대해 조금 더 자세하게 알고 싶다면\n",
        "    - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "    - [번역](https://nlpinkorean.github.io/illustrated-transformer/)\n",
        "    - [Paper](https://arxiv.org/pdf/1706.03762.pdf) (Attention is All You Need)\n",
        "\n",
        "- GPT에 대해 더 자세하게 알고 싶다면\n",
        "    - [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/) (Visualizing Transformer Language Models)\n",
        "    - [Paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) (Improving Language Understanding by Generative Pre-Training)\n",
        "\n",
        "- BERT에 대해 더 자세하게 알고 싶다면\n",
        "    - [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/) (How NLP Cracked Transfer Learning)\n",
        "    - [번역](https://nlpinkorean.github.io/illustrated-bert/)\n",
        "    - [Paper](https://arxiv.org/pdf/1810.04805.pdf) (Pre-training of Deep Bidirectional Transformers for\n",
        "Language Understanding)"
      ]
    }
  ]
}